{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risk & Fraud Individual Assignment by Mohamed Khanafer \n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "# Table of Content\n",
    "___\n",
    "## 1. Loading the libraries\n",
    "\n",
    "___\n",
    "## 2. Objectives and Data Description\n",
    "### 2.1 Goal of the project \n",
    "### 2.2 Dataset description\n",
    "### 2.3 Loading the datasets\n",
    "\n",
    "___\n",
    "## 3. Building a Baseline\n",
    "### 3.1 Metrics used in the notebook\n",
    "### 3.2 Assessing Various algorithms\n",
    "#### 3.2.1 Conclusion on the models\n",
    "\n",
    "___\n",
    "## 4. Hyper-parameters tunning\n",
    "### 4.1 Manual Search tunning\n",
    "### 4.2 Other hyper parameter techniques\n",
    "\n",
    "___\n",
    "## 5. Changing weights: increasing the influence of fraud cases \n",
    "### 5.1 Random Resampling \n",
    "#### 5.1.1 Oversampling\n",
    "#### 5.1.2 Undersampling\n",
    "### 5.2 Manually Duplicating the fraudulant cases\n",
    "### 5.3 Conclusions on the weights rebalancing\n",
    "\n",
    "___\n",
    "## 6.  Feature Importance and Feature Selection \n",
    "### 6.1 Features' Importance according to Scikit-Learn\n",
    "### 6.2 Feature Importance by permutation\n",
    "### 6.3 Feature Selection based on the PSI\n",
    "\n",
    "___\n",
    "## 7. Dealing with the NAs\n",
    "### 7.1 Replacing NAs with the mean\n",
    "### 7.2 Replacing NAs with the median\n",
    "### 7.3 Replacing NAs using the KNN Imputer\n",
    "### 7.4 Replacing NAs manually with a probability\n",
    "\n",
    "___\n",
    "## 8. Some Preprocessing steps \n",
    "### 8.1 Scaling with MinMaxScaler\n",
    "### 8.2 Fixing Skewness\n",
    "### 8.3 Binning the continuous variables\n",
    "### 8.4 Dummy encoding the categorical variables \n",
    "\n",
    "___\n",
    "## 9. Feature Creation \n",
    "### 9.1 Polynomial Features \n",
    "\n",
    "___\n",
    "## 10. Combining insights for the  Models\n",
    "### 10.1 XGBClassifier \n",
    "### 10.2 CatBoost Classifier \n",
    "### 10.3 RandomForest Classifier\n",
    "### 10.4 Ensemble Models\n",
    "\n",
    "___\n",
    "## 11. Conclusion on the final model chosen\n",
    "\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.output_png {display: table-cell;text-align: center;vertical-align: middle;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Basic libraries:\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# For modelling:\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from catboost import CatBoost\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from rfpimp import permutation_importances\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# For Resampling:\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# For Skewness:\n",
    "from scipy.stats import skew, boxcox_normmax\n",
    "from scipy.special import boxcox1p\n",
    "\n",
    "# For Disply:\n",
    "from IPython.core.display import display,HTML\n",
    "display(HTML(\"<style>.output_png {display: table-cell;text-align: center;vertical-align: middle;}</style>\"))\n",
    "pd.set_option('display.max_columns', None)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "def blockPrint():\n",
    "    sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "# For the server:\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "url = 'http://mfalonso.pythonanywhere.com/api/v1.0/uploadpredictions'\n",
    "\n",
    "# For basic profiling:\n",
    "# from pandas_profiling import ProfileReport\n",
    "# # this will be removed later on but it is a great way to visualize and understand the data:\n",
    "# profile = ProfileReport(df, title='Pandas Profiling Report', html={'style':{'full_width':True}})\n",
    "# profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Objectives and Data Description\n",
    "\n",
    "## 2.1 Goal of the project \n",
    "The company analyzed for this problem is a startup company from Peru with around 800 customers initially in their first year and which grew in the next two years. Our goal here is to investigate fraud inside the company. \n",
    "\n",
    "We are given data for 3 years and are asked to predict fraudulant transactions for this startup.\n",
    "\n",
    "The approach I follow in my discovery for the best predictive model are roughly organized around the following steps:\n",
    "1. Sampling and Performance Definition \n",
    "2. Model Evaluation: KS and Gini \n",
    "3. Models Trials: 10 different methods \n",
    "4. Hyperparameter tunning \n",
    "5. Feature Selection \n",
    "6. Preprocessing the variables\n",
    "\n",
    "In the various sections, I experience some ideas and conclude on them before in the last section accumulating all the insights into a final model.\n",
    "\n",
    "I here make a great use of the starter code as well as other codes provided by Professor Manoel Gadi.\n",
    "\n",
    "## 2.2 Dataset description\n",
    "For this problem, we are not given typical train/test datasets. Instead, we have a Development dataset and an Out Of Time dataset that comes for the next 2 years after the Development one (also, the Out Of Time data is 3 times the size of Development sample).\n",
    "\n",
    "The variables in the data do not include the variable names but we know the variable types.\n",
    "\n",
    "We are given 81 different variables in our dataset. And as mentionned are divided into the following 4 types of variables:\n",
    "\n",
    "1. **input binary**: these are flag 0/1 variables from ib_var1 to ib_var21;\n",
    "2. **input categorical nominal**: these are categorical variables and they go from icn_var_22 to icn_var_24;\n",
    "3. **input categorical ordinal**: these are categorical variables in which the order matters and they go from ico_var_25 to icn_var_64;\n",
    "4. **input numerical continuous**: these are input floats going from if_var_65 to if_var_81.\n",
    "\n",
    "In the Development dataset, we have the target variable as well in the ob_target column (a binary column with 1 meaning it is a fraud case and 0 a non-fraud case).\n",
    "\n",
    "## 2.3 Loading the datasets\n",
    "We load the 2 datasets as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Development sample\n",
    "df = pd.read_csv(\"dev.csv\") \n",
    "\n",
    "## Out-of-time sample\n",
    "dfo = pd.read_csv(\"oot0.csv\")\n",
    "\n",
    "## Checking the dimensions\n",
    "print(\"The Development dataframe has \" + str(df.shape[0]) + \" rows and \" +str(df.shape[1]) + \" columns.\")\n",
    "print(\"The Out of Time dataframe has \" + str(dfo.shape[0]) + \" rows and \" +str(dfo.shape[1]) + \" columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the data here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ib_var_1</th>\n",
       "      <th>ib_var_2</th>\n",
       "      <th>ib_var_3</th>\n",
       "      <th>ib_var_4</th>\n",
       "      <th>ib_var_5</th>\n",
       "      <th>ib_var_6</th>\n",
       "      <th>ib_var_7</th>\n",
       "      <th>ib_var_8</th>\n",
       "      <th>ib_var_9</th>\n",
       "      <th>ib_var_10</th>\n",
       "      <th>ib_var_11</th>\n",
       "      <th>ib_var_12</th>\n",
       "      <th>ib_var_13</th>\n",
       "      <th>ib_var_14</th>\n",
       "      <th>ib_var_15</th>\n",
       "      <th>ib_var_16</th>\n",
       "      <th>ib_var_17</th>\n",
       "      <th>ib_var_18</th>\n",
       "      <th>ib_var_19</th>\n",
       "      <th>ib_var_20</th>\n",
       "      <th>ib_var_21</th>\n",
       "      <th>icn_var_22</th>\n",
       "      <th>icn_var_23</th>\n",
       "      <th>icn_var_24</th>\n",
       "      <th>ico_var_25</th>\n",
       "      <th>ico_var_26</th>\n",
       "      <th>ico_var_27</th>\n",
       "      <th>ico_var_28</th>\n",
       "      <th>ico_var_29</th>\n",
       "      <th>ico_var_30</th>\n",
       "      <th>ico_var_31</th>\n",
       "      <th>ico_var_32</th>\n",
       "      <th>ico_var_33</th>\n",
       "      <th>ico_var_34</th>\n",
       "      <th>ico_var_35</th>\n",
       "      <th>ico_var_36</th>\n",
       "      <th>ico_var_37</th>\n",
       "      <th>ico_var_38</th>\n",
       "      <th>ico_var_39</th>\n",
       "      <th>ico_var_40</th>\n",
       "      <th>ico_var_41</th>\n",
       "      <th>ico_var_42</th>\n",
       "      <th>ico_var_43</th>\n",
       "      <th>ico_var_44</th>\n",
       "      <th>ico_var_45</th>\n",
       "      <th>ico_var_46</th>\n",
       "      <th>ico_var_47</th>\n",
       "      <th>ico_var_48</th>\n",
       "      <th>ico_var_49</th>\n",
       "      <th>ico_var_50</th>\n",
       "      <th>ico_var_51</th>\n",
       "      <th>ico_var_52</th>\n",
       "      <th>ico_var_53</th>\n",
       "      <th>ico_var_54</th>\n",
       "      <th>ico_var_55</th>\n",
       "      <th>ico_var_56</th>\n",
       "      <th>ico_var_57</th>\n",
       "      <th>ico_var_58</th>\n",
       "      <th>ico_var_59</th>\n",
       "      <th>ico_var_60</th>\n",
       "      <th>ico_var_61</th>\n",
       "      <th>ico_var_62</th>\n",
       "      <th>ico_var_63</th>\n",
       "      <th>ico_var_64</th>\n",
       "      <th>if_var_65</th>\n",
       "      <th>if_var_66</th>\n",
       "      <th>if_var_67</th>\n",
       "      <th>if_var_68</th>\n",
       "      <th>if_var_69</th>\n",
       "      <th>if_var_70</th>\n",
       "      <th>if_var_71</th>\n",
       "      <th>if_var_72</th>\n",
       "      <th>if_var_73</th>\n",
       "      <th>if_var_74</th>\n",
       "      <th>if_var_75</th>\n",
       "      <th>if_var_76</th>\n",
       "      <th>if_var_77</th>\n",
       "      <th>if_var_78</th>\n",
       "      <th>if_var_79</th>\n",
       "      <th>if_var_80</th>\n",
       "      <th>if_var_81</th>\n",
       "      <th>ob_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>100</td>\n",
       "      <td>6</td>\n",
       "      <td>4626.0</td>\n",
       "      <td>7196.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>9.4634</td>\n",
       "      <td>5140.0</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>822.4</td>\n",
       "      <td>2981.2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.925</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>7.6341</td>\n",
       "      <td>2570.0</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  ib_var_1  ib_var_2  ib_var_3  ib_var_4  ib_var_5  ib_var_6  ib_var_7  \\\n",
       "0   1         0         1         0         0         1         1         0   \n",
       "1   2         0         1         0         0         0         1         0   \n",
       "\n",
       "   ib_var_8  ib_var_9  ib_var_10  ib_var_11  ib_var_12  ib_var_13  ib_var_14  \\\n",
       "0         0         0          0          0          1          0          1   \n",
       "1         0         0          0          0          1          0          0   \n",
       "\n",
       "   ib_var_15  ib_var_16  ib_var_17  ib_var_18  ib_var_19  ib_var_20  \\\n",
       "0          1          1          0          1          1          1   \n",
       "1          1          1          1          0          1          1   \n",
       "\n",
       "   ib_var_21  icn_var_22  icn_var_23  icn_var_24  ico_var_25  ico_var_26  \\\n",
       "0          0           2           1           2           4           4   \n",
       "1          1           3           1           1           5           5   \n",
       "\n",
       "   ico_var_27  ico_var_28  ico_var_29  ico_var_30  ico_var_31  ico_var_32  \\\n",
       "0           4           4           5           4           4           3   \n",
       "1           4           5           4           4           5           5   \n",
       "\n",
       "   ico_var_33  ico_var_34  ico_var_35  ico_var_36  ico_var_37  ico_var_38  \\\n",
       "0          11          15           7          14           3           3   \n",
       "1           3          12           3           5           5           1   \n",
       "\n",
       "   ico_var_39  ico_var_40  ico_var_41  ico_var_42  ico_var_43  ico_var_44  \\\n",
       "0           1           3           4           1           3           4   \n",
       "1           1           2           2           1           4           5   \n",
       "\n",
       "   ico_var_45  ico_var_46  ico_var_47  ico_var_48  ico_var_49  ico_var_50  \\\n",
       "0           4           4           4           4           2           1   \n",
       "1           1           5           1           5           1           3   \n",
       "\n",
       "   ico_var_51  ico_var_52  ico_var_53  ico_var_54  ico_var_55  ico_var_56  \\\n",
       "0           3           3           2           4           2           3   \n",
       "1           2           1           2           4           1           3   \n",
       "\n",
       "   ico_var_57  ico_var_58  ico_var_59  ico_var_60  ico_var_61  ico_var_62  \\\n",
       "0           1           4           3           3           2           4   \n",
       "1           1           4           1           2           2           5   \n",
       "\n",
       "   ico_var_63  ico_var_64  if_var_65  if_var_66  if_var_67  if_var_68  \\\n",
       "0           3           1         31        100          6     4626.0   \n",
       "1           1           1         34        100          5      822.4   \n",
       "\n",
       "   if_var_69  if_var_70  if_var_71  if_var_72  if_var_73  if_var_74  \\\n",
       "0     7196.0          1          6          6      0.800          0   \n",
       "1     2981.2          1          3          2      0.925          5   \n",
       "\n",
       "   if_var_75  if_var_76  if_var_77  if_var_78  if_var_79  if_var_80  \\\n",
       "0          6          5        0.5     9.4634     5140.0   0.766667   \n",
       "1          8          5        0.4     7.6341     2570.0   0.700000   \n",
       "\n",
       "   if_var_81  ob_target  \n",
       "0          1          0  \n",
       "1          4          0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ib_var_1</th>\n",
       "      <th>ib_var_2</th>\n",
       "      <th>ib_var_3</th>\n",
       "      <th>ib_var_4</th>\n",
       "      <th>ib_var_5</th>\n",
       "      <th>ib_var_6</th>\n",
       "      <th>ib_var_7</th>\n",
       "      <th>ib_var_8</th>\n",
       "      <th>ib_var_9</th>\n",
       "      <th>ib_var_10</th>\n",
       "      <th>ib_var_11</th>\n",
       "      <th>ib_var_12</th>\n",
       "      <th>ib_var_13</th>\n",
       "      <th>ib_var_14</th>\n",
       "      <th>ib_var_15</th>\n",
       "      <th>ib_var_16</th>\n",
       "      <th>ib_var_17</th>\n",
       "      <th>ib_var_18</th>\n",
       "      <th>ib_var_19</th>\n",
       "      <th>ib_var_20</th>\n",
       "      <th>ib_var_21</th>\n",
       "      <th>icn_var_22</th>\n",
       "      <th>icn_var_23</th>\n",
       "      <th>icn_var_24</th>\n",
       "      <th>ico_var_25</th>\n",
       "      <th>ico_var_26</th>\n",
       "      <th>ico_var_27</th>\n",
       "      <th>ico_var_28</th>\n",
       "      <th>ico_var_29</th>\n",
       "      <th>ico_var_30</th>\n",
       "      <th>ico_var_31</th>\n",
       "      <th>ico_var_32</th>\n",
       "      <th>ico_var_33</th>\n",
       "      <th>ico_var_34</th>\n",
       "      <th>ico_var_35</th>\n",
       "      <th>ico_var_36</th>\n",
       "      <th>ico_var_37</th>\n",
       "      <th>ico_var_38</th>\n",
       "      <th>ico_var_39</th>\n",
       "      <th>ico_var_40</th>\n",
       "      <th>ico_var_41</th>\n",
       "      <th>ico_var_42</th>\n",
       "      <th>ico_var_43</th>\n",
       "      <th>ico_var_44</th>\n",
       "      <th>ico_var_45</th>\n",
       "      <th>ico_var_46</th>\n",
       "      <th>ico_var_47</th>\n",
       "      <th>ico_var_48</th>\n",
       "      <th>ico_var_49</th>\n",
       "      <th>ico_var_50</th>\n",
       "      <th>ico_var_51</th>\n",
       "      <th>ico_var_52</th>\n",
       "      <th>ico_var_53</th>\n",
       "      <th>ico_var_54</th>\n",
       "      <th>ico_var_55</th>\n",
       "      <th>ico_var_56</th>\n",
       "      <th>ico_var_57</th>\n",
       "      <th>ico_var_58</th>\n",
       "      <th>ico_var_59</th>\n",
       "      <th>ico_var_60</th>\n",
       "      <th>ico_var_61</th>\n",
       "      <th>ico_var_62</th>\n",
       "      <th>ico_var_63</th>\n",
       "      <th>ico_var_64</th>\n",
       "      <th>if_var_65</th>\n",
       "      <th>if_var_66</th>\n",
       "      <th>if_var_67</th>\n",
       "      <th>if_var_68</th>\n",
       "      <th>if_var_69</th>\n",
       "      <th>if_var_70</th>\n",
       "      <th>if_var_71</th>\n",
       "      <th>if_var_72</th>\n",
       "      <th>if_var_73</th>\n",
       "      <th>if_var_74</th>\n",
       "      <th>if_var_75</th>\n",
       "      <th>if_var_76</th>\n",
       "      <th>if_var_77</th>\n",
       "      <th>if_var_78</th>\n",
       "      <th>if_var_79</th>\n",
       "      <th>if_var_80</th>\n",
       "      <th>if_var_81</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>100</td>\n",
       "      <td>8</td>\n",
       "      <td>7196.0</td>\n",
       "      <td>5140.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.775</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5140.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>100</td>\n",
       "      <td>6</td>\n",
       "      <td>514.0</td>\n",
       "      <td>5140.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>12.9024</td>\n",
       "      <td>5140.0</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  ib_var_1  ib_var_2  ib_var_3  ib_var_4  ib_var_5  ib_var_6  ib_var_7  \\\n",
       "0   1         0         1         0         0         1         0         0   \n",
       "1   2         1         0         0         0         1         1         0   \n",
       "\n",
       "   ib_var_8  ib_var_9  ib_var_10  ib_var_11  ib_var_12  ib_var_13  ib_var_14  \\\n",
       "0         0         0          0          0          1          0          1   \n",
       "1         0         0          0          0          1          0          1   \n",
       "\n",
       "   ib_var_15  ib_var_16  ib_var_17  ib_var_18  ib_var_19  ib_var_20  \\\n",
       "0        0.0        0.0        1.0        0.0        1.0        0.0   \n",
       "1        1.0        1.0        1.0        0.0        0.0        1.0   \n",
       "\n",
       "   ib_var_21  icn_var_22  icn_var_23  icn_var_24  ico_var_25  ico_var_26  \\\n",
       "0        1.0         2.0           1         1.0           4         4.0   \n",
       "1        0.0         2.0           5         2.0           4         3.0   \n",
       "\n",
       "   ico_var_27  ico_var_28  ico_var_29  ico_var_30  ico_var_31  ico_var_32  \\\n",
       "0           4           4           4           3           4           4   \n",
       "1           4           4           4           4           3           4   \n",
       "\n",
       "   ico_var_33  ico_var_34  ico_var_35  ico_var_36  ico_var_37  ico_var_38  \\\n",
       "0         NaN           0           0           0         4.0           4   \n",
       "1         1.0           0           0           0         3.0           3   \n",
       "\n",
       "   ico_var_39  ico_var_40  ico_var_41  ico_var_42  ico_var_43  ico_var_44  \\\n",
       "0           2           3           3           4           4           4   \n",
       "1           3           3           3           3           4           4   \n",
       "\n",
       "   ico_var_45  ico_var_46  ico_var_47  ico_var_48  ico_var_49  ico_var_50  \\\n",
       "0           2           4           2           4           2           2   \n",
       "1           3           4           3           4           1           3   \n",
       "\n",
       "   ico_var_51  ico_var_52  ico_var_53  ico_var_54  ico_var_55  ico_var_56  \\\n",
       "0           2           4           2           4           4           4   \n",
       "1           3           3           3           3           1           2   \n",
       "\n",
       "   ico_var_57  ico_var_58  ico_var_59  ico_var_60  ico_var_61  ico_var_62  \\\n",
       "0           2           3           2           2           2           2   \n",
       "1           2           2           3           2           2           3   \n",
       "\n",
       "   ico_var_63  ico_var_64  if_var_65  if_var_66  if_var_67  if_var_68  \\\n",
       "0           3           2         43        100          8     7196.0   \n",
       "1           2           2         31        100          6      514.0   \n",
       "\n",
       "   if_var_69  if_var_70  if_var_71  if_var_72  if_var_73  if_var_74  \\\n",
       "0     5140.0          1          4          5      0.775        1.0   \n",
       "1     5140.0          1          1          6      0.750        0.0   \n",
       "\n",
       "   if_var_75  if_var_76  if_var_77  if_var_78  if_var_79  if_var_80  if_var_81  \n",
       "0       10.0       11.0   0.666667        NaN     5140.0   0.666667          3  \n",
       "1        8.0       10.0   0.600000    12.9024     5140.0   0.733333          0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfo.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables studied are the following:\n",
    "\n",
    "\n",
    "| Binary Inputs | Categorical Nominal Inputs | Categorical Ordinal Inputs | Continuous Numerical Inputs |\n",
    "| --- | --- | --- | --- |\n",
    "| ib_var1 | icn_var_22 |ico_var_25 | if_var_65 |\n",
    "| ib_var2 | icn_var_23 |ico_var_26 | if_var_66 |\n",
    "| ib_var3 | icn_var_24 |ico_var_27 | if_var_67 |\n",
    "| ib_var4 | |ico_var_28 | if_var_68 |\n",
    "| ib_var5 | |ico_var_29 | if_var_69 |\n",
    "| ib_var6 | |ico_var_30 | if_var_70 |\n",
    "| ib_var7 | |ico_var_31 | if_var_71 |\n",
    "| ib_var8 | |ico_var_32 | if_var_72 |\n",
    "| ib_var9 | |ico_var_33 | if_var_73 |\n",
    "| ib_var10| |ico_var_34 | if_var_74 |\n",
    "| ib_var11| |ico_var_35 | if_var_75 |\n",
    "| ib_var12 | |ico_var_36 | if_var_76 |\n",
    "| ib_var13 | |ico_var_37 | if_var_77 |\n",
    "| ib_var14 | |ico_var_38 | if_var_78 |\n",
    "| ib_var15 | |ico_var_39 | if_var_79 |\n",
    "| ib_var16 | |ico_var_40 | if_var_80 |\n",
    "| ib_var17 | |ico_var_41 | if_var_81 |\n",
    "| ib_var18 | |ico_var_42 | if_var_82 |\n",
    "| ib_var19 | |ico_var_43 |  |\n",
    "| ib_var20 | |ico_var_44 |  |\n",
    "| ib_var21 | |ico_var_45 |  |\n",
    "| ib_var22 | |**...** |  |\n",
    "|  | |ico_var_64 |  |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Building a Baseline\n",
    "With a basic understanding of the data, we are now ready to start modelling. The first step is to evaluate the performance of various algorithms to see which one will perform best on our data.\n",
    "But first, we look at the metrics we will be using for evaluating our models throughout the script:\n",
    "\n",
    "## 3.1 Metrics used in the notebook\n",
    "The 2 main metrics used here to assess the performance of our models are the Gini and the KS scores. \n",
    "\n",
    "As mentionned by Idan Schatz in his [article](https://towardsdatascience.com/using-the-gini-coefficient-to-evaluate-the-performance-of-credit-score-models-59fe13ef420), the Gini or the Gini coefficient is one of the most used metrics when it comes to evaluating the performance of credit score models as well as other models in the Financial sector. It indicates the modelâ€™s discriminatory power, how effective is the the model when it comes to differentiate between the fraudulant and non-fraudulant transactions. It has been shown that there was a linear relationship between the AUC and the Gini coefficient, thus here we derive the Gini from the AUC score using the formula **Gini = 2 * AUC-1**. \n",
    "\n",
    "And as said by [Ratnakar Pandey](https://www.quora.com/Which-one-is-better-to-evaluate-a-logistic-regression-Gini-KS-or-ROC#), the KS (Kolmogorov-Smirnov) is similar to Gini, in that it also captures this discriminatory power of the model in its ability to separate the fraudulant from the non-faudulant ones. It represents *the highest separation between the Cumulative Good Rate and Cumulative Bad Rate*. The higher the KS, the better the model is, indicating a higher separation between those two classes. For calculating it, we here define its function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The code that calculates Gini:\n",
    "# gini_score = 2*roc_auc_score(y, pred_dev)-1\n",
    "# print (\"GINI DEVELOPMENT=\", gini_score)\n",
    "\n",
    "## The code that calculates KS:\n",
    "def KS(b,a):  \n",
    "    try:\n",
    "        tot_bads=1.0*sum(b)\n",
    "        tot_goods=1.0*(len(b)-tot_bads)\n",
    "        elements = zip(*[a,b])\n",
    "        elements = sorted(elements,key= lambda x: x[0])\n",
    "        elements_df = pd.DataFrame({'probability': b,'gbi': a})\n",
    "        pivot_elements_df = pd.pivot_table(elements_df, values='probability', index=['gbi'], aggfunc=[sum,len]).fillna(0)\n",
    "        max_ks = perc_goods = perc_bads = cum_perc_bads = cum_perc_goods = 0\n",
    "        for i in range(len(pivot_elements_df)):\n",
    "            perc_goods =  (pivot_elements_df.iloc[i]['len'] - pivot_elements_df.iloc[i]['sum']) / tot_goods\n",
    "            perc_bads = pivot_elements_df.iloc[i]['sum']/ tot_bads\n",
    "            cum_perc_goods += perc_goods\n",
    "            cum_perc_bads += perc_bads\n",
    "            A = cum_perc_bads-cum_perc_goods\n",
    "            if abs(A['probability']) > max_ks:\n",
    "                max_ks = abs(A['probability'])\n",
    "    except:\n",
    "        max_ks = 0\n",
    "    return max_ks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Assessing Various algorithms\n",
    "Before jumping into further processing, it is useful seeing which models would perform the best for our problem. I used here the great guide for Classifier algorithms by an [IBM tutorial](https://developer.ibm.com/tutorials/learn-classification-algorithms-using-python-and-scikit-learn/#set-up) in scikit-learn. I thus here assess those various models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Until we define a better approach, we fill in the nulls with 0:\n",
    "df = df.fillna(0)\n",
    "dfo = dfo.fillna(0)\n",
    "\n",
    "## Before doing any feature selection, we reun models with all the variables\n",
    "in_model = ['ib_var_1', 'ib_var_2', 'ib_var_3', 'ib_var_4', 'ib_var_5', 'ib_var_6', 'ib_var_7', 'ib_var_8', 'ib_var_9', 'ib_var_10','ib_var_11', 'ib_var_12', 'ib_var_13', 'ib_var_14', 'ib_var_15','ib_var_16', 'ib_var_17', 'ib_var_18', 'ib_var_19', 'ib_var_20','ib_var_21', 'icn_var_22', 'icn_var_23', 'icn_var_24', 'ico_var_25','ico_var_26', 'ico_var_27', 'ico_var_28', 'ico_var_29', 'ico_var_30','ico_var_31', 'ico_var_32', 'ico_var_33', 'ico_var_34', 'ico_var_35','ico_var_36', 'ico_var_37', 'ico_var_38', 'ico_var_39', 'ico_var_40','ico_var_41', 'ico_var_42', 'ico_var_43', 'ico_var_44', 'ico_var_45','ico_var_46', 'ico_var_47', 'ico_var_48', 'ico_var_49', 'ico_var_50',\n",
    "       'ico_var_51', 'ico_var_52', 'ico_var_53', 'ico_var_54', 'ico_var_55','ico_var_56', 'ico_var_57', 'ico_var_58', 'ico_var_59', 'ico_var_60','ico_var_61', 'ico_var_62', 'ico_var_63', 'ico_var_64', 'if_var_65','if_var_66', 'if_var_67', 'if_var_68', 'if_var_69', 'if_var_70','if_var_71', 'if_var_72', 'if_var_73', 'if_var_74', 'if_var_75','if_var_76', 'if_var_77', 'if_var_78', 'if_var_79', 'if_var_80','if_var_81']\n",
    "output_var = 'ob_target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We define the X and Y from the feature selected:\n",
    "X = df[in_model]\n",
    "y = df[output_var]\n",
    "Xo = dfo[in_model] ##x out of time\n",
    "\n",
    "## Model 1: Logistic Regression\n",
    "clf = LogisticRegression(random_state=5, solver='lbfgs')\n",
    "pred_dev_LR = clf.fit(X, y).predict_proba(X)[:,1]\n",
    "pred_oot_LR  = clf.fit(X, y).predict_proba(Xo)[:,1]\n",
    "gini_score_LR = 2*roc_auc_score(y, pred_dev_LR)-1\n",
    "KS_score_LR = KS(y,pred_dev_LR)\n",
    "\n",
    "## Model: Naive Bayes\n",
    "nb = MultinomialNB()\n",
    "pred_dev_NB = nb.fit(X, y).predict_proba(X)[:,1]\n",
    "pred_oot_NB  = nb.fit(X, y).predict_proba(Xo)[:,1]\n",
    "gini_score_NB = 2*roc_auc_score(y, pred_dev_NB)-1\n",
    "KS_score_NB = KS(y,pred_dev_NB)\n",
    "\n",
    "## Model: KNN \n",
    "knn = KNeighborsClassifier(n_neighbors = 2)\n",
    "pred_dev_KNN = knn.fit(X, y).predict_proba(X)[:,1]\n",
    "pred_oot_KNN  = knn.fit(X, y).predict_proba(Xo)[:,1]\n",
    "gini_score_KNN = 2*roc_auc_score(y, pred_dev_KNN)-1\n",
    "KS_score_KNN = KS(y,pred_dev_KNN)\n",
    "    \n",
    "## Model: SVM\n",
    "svm = SVC(kernel='rbf', gamma= 'auto', random_state=5, probability=True)\n",
    "pred_dev_SVM = svm.fit(X, y).predict_proba(X)[:,1]\n",
    "pred_oot_SVM  = svm.fit(X, y).predict_proba(Xo)[:,1]\n",
    "gini_score_SVM = 2*roc_auc_score(y, pred_dev_SVM)-1\n",
    "KS_score_SVM = KS(y,pred_dev_SVM)\n",
    "\n",
    "## Model: Random Forrest\n",
    "rf = RandomForestClassifier(random_state=5)\n",
    "pred_dev_RF = rf.fit(X, y).predict_proba(X)[:,1]\n",
    "pred_oot_RF  = rf.fit(X, y).predict_proba(Xo)[:,1]\n",
    "gini_score_RF = 2*roc_auc_score(y, pred_dev_RF)-1\n",
    "KS_score_RF = KS(y,pred_dev_RF)\n",
    "\n",
    "## Model: XGBClassifier\n",
    "xgbcla = XGBClassifier(objective='binary:logistic', random_state=5)\n",
    "pred_dev_XGBCla = xgbcla.fit(X, y).predict_proba(X)[:,1]\n",
    "pred_oot_XGBCla  = xgbcla.fit(X, y).predict_proba(Xo)[:,1]\n",
    "gini_score_XGBCla = 2*roc_auc_score(y, pred_dev_XGBCla)-1\n",
    "KS_score_XGBCla = KS(y,pred_dev_XGBCla)\n",
    "\n",
    "## Model: LGBMClassifier\n",
    "lgbm = LGBMClassifier(random_state=5)\n",
    "pred_dev_LGBM = lgbm.fit(X, y).predict_proba(X)[:,1]\n",
    "pred_oot_LGBM  = lgbm.fit(X, y).predict_proba(Xo)[:,1]\n",
    "gini_score_LGBM = 2*roc_auc_score(y, pred_dev_LGBM)-1\n",
    "KS_score_LGBM = KS(y,pred_dev_LGBM)\n",
    "\n",
    "## Model: Neural Network\n",
    "neunet = MLPClassifier(random_state =5)\n",
    "pred_dev_NeuNet = neunet.fit(X, y).predict_proba(X)[:,1]\n",
    "pred_oot_NeuNet  = neunet.fit(X, y).predict_proba(Xo)[:,1]\n",
    "gini_score_NeuNet = 2*roc_auc_score(y, pred_dev_NeuNet)-1\n",
    "KS_score_NeuNet = KS(y,pred_dev_NeuNet)\n",
    "\n",
    "## Model: Logistic SMOTE\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(random_state=5), LogisticRegression(C=1e5))\n",
    "pred_dev_SMOTE = smote_pipeline.fit(X, y).predict_proba(X)[:,1]\n",
    "pred_oot_SMOTE  = smote_pipeline.fit(X, y).predict_proba(Xo)[:,1]\n",
    "gini_score_SMOTE = 2*roc_auc_score(y, pred_dev_SMOTE)-1\n",
    "KS_score_SMOTE = KS(y,pred_dev_SMOTE)\n",
    "\n",
    "## Model: CatBoostClassifier\n",
    "blockPrint()\n",
    "catb = CatBoostClassifier(random_seed=5);\n",
    "pred_dev_CATB = catb.fit(X, y).predict_proba(X)[:,1]\n",
    "pred_oot_CATB  = catb.fit(X, y).predict_proba(Xo)[:,1]\n",
    "gini_score_CATB = 2*roc_auc_score(y, pred_dev_CATB)-1\n",
    "KS_score_CATB = KS(y,pred_dev_CATB);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Conclusion on the models\n",
    "\n",
    "Because we do not have the target varriable in the Out of Time sample, it is not possible to get the metrics on these values. I thus have to rely on the server to get them, I did this and manually added the results to the following table to summarize the performance of the different models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gini Scores</th>\n",
       "      <th>KS scores</th>\n",
       "      <th>KS2 Score Ranking</th>\n",
       "      <th>Gini Score Ranking</th>\n",
       "      <th>Grade Score Ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CatBoost Classifier</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.363965933142</td>\n",
       "      <td>0.454567509866</td>\n",
       "      <td>8.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forrest</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.360761190029</td>\n",
       "      <td>0.46946020758</td>\n",
       "      <td>8.549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBM Classifier</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.313320825516</td>\n",
       "      <td>0.365285261416</td>\n",
       "      <td>7.425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB Classifier</th>\n",
       "      <td>0.979586</td>\n",
       "      <td>0.906865</td>\n",
       "      <td>0.300132163883</td>\n",
       "      <td>0.373023318145</td>\n",
       "      <td>7.112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.387008</td>\n",
       "      <td>0.338449</td>\n",
       "      <td>0.171761060638</td>\n",
       "      <td>0.202321648074</td>\n",
       "      <td>4.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K-nearest neighbors</th>\n",
       "      <td>0.916559</td>\n",
       "      <td>0.904639</td>\n",
       "      <td>0.165535263727</td>\n",
       "      <td>0.17026728528</td>\n",
       "      <td>3.923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.130330</td>\n",
       "      <td>0.132849</td>\n",
       "      <td>0.110686096914</td>\n",
       "      <td>0.116892254087</td>\n",
       "      <td>2.623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression SMOTE</th>\n",
       "      <td>0.310069</td>\n",
       "      <td>0.257381</td>\n",
       "      <td>0.106024547362</td>\n",
       "      <td>0.124552907143</td>\n",
       "      <td>2.512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP Neural Network</th>\n",
       "      <td>0.175668</td>\n",
       "      <td>0.140933</td>\n",
       "      <td>0.0914079612566</td>\n",
       "      <td>0.086709442786</td>\n",
       "      <td>2.166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Vector Machine</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0236323810756</td>\n",
       "      <td>0.0372646697289</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Gini Scores  KS scores KS2 Score Ranking  \\\n",
       "CatBoost Classifier           1.000000   1.000000    0.363965933142   \n",
       "Random Forrest                1.000000   1.000000    0.360761190029   \n",
       "LGBM Classifier               1.000000   1.000000    0.313320825516   \n",
       "XGB Classifier                0.979586   0.906865    0.300132163883   \n",
       "Logistic Regression           0.387008   0.338449    0.171761060638   \n",
       "K-nearest neighbors           0.916559   0.904639    0.165535263727   \n",
       "Naive Bayes                   0.130330   0.132849    0.110686096914   \n",
       "Logistic Regression SMOTE     0.310069   0.257381    0.106024547362   \n",
       "MLP Neural Network            0.175668   0.140933   0.0914079612566   \n",
       "Support Vector Machine       -1.000000   1.000000   0.0236323810756   \n",
       "\n",
       "                          Gini Score Ranking Grade Score Ranking  \n",
       "CatBoost Classifier           0.454567509866               8.625  \n",
       "Random Forrest                 0.46946020758               8.549  \n",
       "LGBM Classifier               0.365285261416               7.425  \n",
       "XGB Classifier                0.373023318145               7.112  \n",
       "Logistic Regression           0.202321648074                4.07  \n",
       "K-nearest neighbors            0.17026728528               3.923  \n",
       "Naive Bayes                   0.116892254087               2.623  \n",
       "Logistic Regression SMOTE     0.124552907143               2.512  \n",
       "MLP Neural Network            0.086709442786               2.166  \n",
       "Support Vector Machine       0.0372646697289                0.56  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame({'Gini Scores':[gini_score_LR, gini_score_NB, gini_score_KNN, gini_score_SVM, gini_score_RF, gini_score_XGBCla, gini_score_LGBM,gini_score_NeuNet, gini_score_SMOTE, gini_score_CATB],\n",
    "                        'KS scores': [KS_score_LR, KS_score_NB, KS_score_KNN, KS_score_SVM, KS_score_RF, KS_score_XGBCla, KS_score_LGBM,KS_score_NeuNet, KS_score_SMOTE,KS_score_CATB]\n",
    "                       , 'KS2 Score Ranking': [\"0.171761060638\",\"0.110686096914\",\"0.165535263727\",\"0.0236323810756\",\"0.360761190029\",\"0.300132163883\",\"0.313320825516\",\"0.0914079612566\",\"0.106024547362\",\"0.363965933142\"]\n",
    "                        , 'Gini Score Ranking': [\"0.202321648074\",\"0.116892254087\", \"0.17026728528\",\"0.0372646697289\", \"0.46946020758\", \"0.373023318145\",\"0.365285261416\",\"0.086709442786\", \"0.124552907143\",\"0.454567509866\"],\n",
    "                        'Grade Score Ranking':[\"4.07\",\"2.623\",\"3.923\", \"0.56\", \"8.549\",\"7.112\", \"7.425\",\"2.166\",\"2.512\",\"8.625\"]\n",
    "                       }, index=[\"Logistic Regression\", \"Naive Bayes\", \"K-nearest neighbors\", \"Support Vector Machine\", \"Random Forrest\", \"XGB Classifier\",\"LGBM Classifier\",\"MLP Neural Network\", \"Logistic Regression SMOTE\", \"CatBoost Classifier\"])\n",
    "\n",
    "results.sort_values(by=[\"KS2 Score Ranking\"], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the table above, the best models for this problem seem to be tree-based models. Even if it is clear that those models are highly overfitting here with perfect metrrics in training but poor performances when it comes to predicting. \n",
    "\n",
    "The models I will thus be focusing on in the first approach are the the Catboost Classifier, the Random Forrest, the LGBM Classifier, and the XGB Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Hyper-parameters tunning \n",
    "I will start by experimenting various approaches with the different shortlisted models as they gave me the best results as seen in the table.\n",
    "As well elaborated in this [article](https://towardsdatascience.com/hyperparameters-optimization-526348bb8e2d) by Pier Paolo Ippolito, the different approaches to hyper parameter optimization could be summarized in the following categories:\n",
    "1. **Manual Search**\n",
    "2. **Random Search**\n",
    "3. **Grid Search**\n",
    "4. **Automated Hyperparameter Tuning** (Bayesian Optimization, Genetic Algorithms)\n",
    "\n",
    "Because here the performance of these algorithms are already very high in training, it seems that hyper parameter optimization maybe could be useless. But I nonetheless try playing with these parameters. \n",
    "\n",
    "I here will use a combination of the approaches to hyper-parameters optimization mentionned above, to try improving the performance of the initial model (KS on the OOT of 0.363) that can be found in the summarized table.\n",
    "\n",
    "## 4.1 Manual Search tunning\n",
    "Looking at the documentation of the CatBoost Classifier in this [article](https://catboost.ai/docs/concepts/python-reference_catboostclassifier.html), I played around with the model's parameters trying to increase the score of the model. Especifically, I try to increase overfitting, which is what we want our model to do for this problem. But as highlighted, the most effective one will be manual search.\n",
    "\n",
    "#### Experience 1: evaluation metric and n_estimators for the Catboost \n",
    "Playing around with the 2 parameters *eval_metric* and *n_eestimators* gave me a significant increase to 9.069 in the grade on the board:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KS2 = 0.382706864204; GINI = 0.478597306814; GRADE = 9.069'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[in_model]\n",
    "y = df[output_var]\n",
    "Xo = dfo[in_model] ##x out of time\n",
    "catb = CatBoostClassifier(random_seed=5, eval_metric = 'Logloss', n_estimators = 1210);\n",
    "pred_dev = catb.fit(X, y).predict_proba(X)[:,1]\n",
    "pred_oot  = catb.fit(X, y).predict_proba(Xo)[:,1]\n",
    "dfo['pred'] = pred_oot\n",
    "dfo_tosend = dfo[list(['id','pred'])]\n",
    "i=1\n",
    "filename = \"student_sub\"+str(i)+\".csv\"\n",
    "dfo_tosend.to_csv(filename, sep=',')\n",
    "files = {'file': (filename, open(filename, 'rb'))}\n",
    "rsub = requests.post(url, files=files, auth=HTTPBasicAuth('mohamed.khanafer', 'mfalonso123'))\n",
    "resp_str = str(rsub.text)\n",
    "resp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playing around with the parameters does significantly improve the performance of the model, highlighting the importance of finding the proper parameters.\n",
    "\n",
    "#### Experience 2: tweaking the parameters of Random Forrest\n",
    "I do the same with the Random Forrest Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KS2 = 0.373473876838; GINI = 0.515840026248; GRADE = 8.85'"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[in_model]\n",
    "y = df[output_var]\n",
    "Xo = dfo[in_model] ##x out of time\n",
    "rf = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, \n",
    "                                              class_weight='balanced_subsample',\n",
    "                                              criterion='gini', max_depth=None,\n",
    "                                              max_features='auto',\n",
    "                                              max_leaf_nodes=None,\n",
    "                                              max_samples=None,\n",
    "                                              min_impurity_decrease=0.0,\n",
    "                                              min_impurity_split=None,\n",
    "                                              min_samples_leaf=1,\n",
    "                                              min_samples_split=2,\n",
    "                                              min_weight_fraction_leaf=0.0,\n",
    "                                              n_estimators=1200, n_jobs=-1,\n",
    "                                              oob_score=False,\n",
    "                                              random_state=5, verbose=0,\n",
    "                                              warm_start=False)\n",
    "\n",
    "\n",
    "pred_dev = rf.fit(X, y).predict_proba(X)[:,1]\n",
    "pred_oot  = rf.fit(X, y).predict_proba(Xo)[:,1]\n",
    "dfo['pred'] = pred_oot\n",
    "dfo_tosend = dfo[list(['id','pred'])]\n",
    "i=1\n",
    "filename = \"student_sub\"+str(i)+\".csv\"\n",
    "dfo_tosend.to_csv(filename, sep=',')\n",
    "files = {'file': (filename, open(filename, 'rb'))}\n",
    "rsub = requests.post(url, files=files, auth=HTTPBasicAuth('mohamed.khanafer', 'mfalonso123'))\n",
    "resp_str = str(rsub.text)\n",
    "resp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, there also is an increase in the performance after adjusting the hyperparameters, thus highlighting the importance of considering them in the future steps.\n",
    "\n",
    "## 4.2 Other hyper parameter techniques \n",
    "I later tried Random Search, Grid Search, and Automated Hyperparameter Tuning without getting better results and thus decided to keep the ones I got with the manual optimization and play with these throughout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Changing weights: increasing the influence of fraud cases \n",
    "\n",
    "Given that the fraudulant cases are only 10% of the total data I thought that increasing the number of fraudulant cases would lead to a better peerformance by the models. \n",
    "\n",
    "I first try Random Resampling to see if it affects the performance of the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Random Resampling \n",
    "As mentionned in this [article](https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/), when resampling, we are actually creating a new transformed version of the training data where the studied variables have a different class distribution, solving the imbalance problem we have for this dataset. And there are 2 ways of doing so:\n",
    "\n",
    "- **Random Oversampling**: we randomly duplicate instances from the minority class (the fraudulant cases here);\n",
    "- **Random Undersampling**: we randomly delete instances in the majority class.\n",
    "\n",
    "### 5.1.1 Oversampling\n",
    "\n",
    "I used the approach and code explained in the above-mentionned article here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of non fraud cases before the resampling are: 776. And the fraud cases are: 88.\n",
      "The number of non fraud cases after the resampling are: 776. And the fraud cases are: 776.\n"
     ]
    }
   ],
   "source": [
    "# Defining the dataset\n",
    "X = df[in_model]\n",
    "y = df[output_var]\n",
    "# Summarizing the class distribution before the transformation\n",
    "print(\"The number of non fraud cases before the resampling are: \" + str(Counter(y)[0]) + \". And the fraud cases are: \" + str(Counter(y)[1])+\".\")\n",
    "# Defining the oversampling strategy\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "# Fitting and applying the transformation\n",
    "X_over, y_over = oversample.fit_resample(X, y)\n",
    "# Summarize the class distribution after the transformation\n",
    "print(\"The number of non fraud cases after the resampling are: \" + str(Counter(y_over)[0]) + \". And the fraud cases are: \" + str(Counter(y_over)[1])+\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have oversampled the data, we can rerun a model to see the change in performance (note that here I remove the hyper parameter *class_weight='balanced_subsample'* because I have changed the weights in the classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KS2 = 0.381763001506; GINI = 0.524467647575; GRADE = 9.047'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X_over\n",
    "y = y_over\n",
    "Xo = dfo[in_model] ##x out of time\n",
    "rf = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, criterion='gini', max_depth=None,\n",
    "                                              max_features='auto',max_leaf_nodes=None,max_samples=None,min_impurity_decrease=0.0,\n",
    "                                              min_impurity_split=None,min_samples_leaf=1,min_samples_split=2,\n",
    "                                              min_weight_fraction_leaf=0.0,n_estimators=1200, n_jobs=-1,\n",
    "                                              oob_score=False,random_state=5, verbose=0,warm_start=False)\n",
    "\n",
    "pred_dev = rf.fit(X, y).predict_proba(X)[:,1]\n",
    "pred_oot  = rf.fit(X, y).predict_proba(Xo)[:,1]\n",
    "dfo['pred'] = pred_oot\n",
    "dfo_tosend = dfo[list(['id','pred'])]\n",
    "i=1\n",
    "filename = \"student_sub\"+str(i)+\".csv\"\n",
    "dfo_tosend.to_csv(filename, sep=',')\n",
    "files = {'file': (filename, open(filename, 'rb'))}\n",
    "rsub = requests.post(url, files=files, auth=HTTPBasicAuth('mohamed.khanafer', 'mfalonso123'))\n",
    "resp_str = str(rsub.text)\n",
    "resp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oversampling did improve the performance of the model to a 0.3817 KS2 as compared to 0.3734 in the previous section. Highlighting thus that this does help the model. I try now underrsampling:\n",
    "\n",
    "### 5.1.2 Undersampling\n",
    "I follow still the same article by Jason Browlee here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of non fraud cases before the resampling are: 776. And the fraud cases are: 88.\n",
      "The number of non fraud cases after the resampling are: 88. And the fraud cases are: 88.\n"
     ]
    }
   ],
   "source": [
    "# Defining the dataset\n",
    "X = df[in_model]\n",
    "y = df[output_var]\n",
    "# Summarizing the class distribution before the transformation\n",
    "print(\"The number of non fraud cases before the resampling are: \" + str(Counter(y)[0]) + \". And the fraud cases are: \" + str(Counter(y)[1])+\".\")\n",
    "# define undersample strategy\n",
    "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "# fit and apply the transform\n",
    "X_under, y_under = undersample.fit_resample(X, y)\n",
    "# Summarize the class distribution after the transformation\n",
    "print(\"The number of non fraud cases after the resampling are: \" + str(Counter(y_under)[0]) + \". And the fraud cases are: \" + str(Counter(y_under)[1])+\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KS2 = 0.30007324467; GINI = 0.406893779055; GRADE = 7.111'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X_under\n",
    "y = y_under\n",
    "Xo = dfo[in_model] ##x out of time\n",
    "rf = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, criterion='gini', max_depth=None,\n",
    "                                              max_features='auto',max_leaf_nodes=None,max_samples=None,min_impurity_decrease=0.0,\n",
    "                                              min_impurity_split=None,min_samples_leaf=1,min_samples_split=2,\n",
    "                                              min_weight_fraction_leaf=0.0,n_estimators=1200, n_jobs=-1,\n",
    "                                              oob_score=False,random_state=5, verbose=0,warm_start=False)\n",
    "\n",
    "pred_dev = rf.fit(X, y).predict_proba(X)[:,1]\n",
    "pred_oot  = rf.fit(X, y).predict_proba(Xo)[:,1]\n",
    "dfo['pred'] = pred_oot\n",
    "dfo_tosend = dfo[list(['id','pred'])]\n",
    "i=1\n",
    "filename = \"student_sub\"+str(i)+\".csv\"\n",
    "dfo_tosend.to_csv(filename, sep=',')\n",
    "files = {'file': (filename, open(filename, 'rb'))}\n",
    "rsub = requests.post(url, files=files, auth=HTTPBasicAuth('mohamed.khanafer', 'mfalonso123'))\n",
    "resp_str = str(rsub.text)\n",
    "resp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undersampling does not seem to be the proper way to deal with the class imbalance here with a decrease in performance. \n",
    "\n",
    "Another way to randomly resampling the data, I could just duplicate the fraudulant cases. This is tried next.\n",
    "\n",
    "## 5.2 Manually Duplicating the fraudulant cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experience 1: tripling the cases of faud + hyper parameter tunning for Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KS2 = 0.388958077247; GINI = 0.477204040703; GRADE = 9.217'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only1 = df[df['ob_target']==1]\n",
    "result = df.append(only1)\n",
    "result = result.append(only1)\n",
    "\n",
    "X = result[in_model]\n",
    "y = result[output_var]\n",
    "Xo = dfo[in_model] ##x out of time\n",
    "catb = CatBoostClassifier(random_seed=5, eval_metric = 'Logloss', iterations = 3600);\n",
    "pred_dev = catb.fit(X, y).predict_proba(X)[:,1]\n",
    "pred_oot  = catb.fit(X, y).predict_proba(Xo)[:,1]\n",
    "dfo['pred'] = pred_oot\n",
    "dfo_tosend = dfo[list(['id','pred'])]\n",
    "i=1\n",
    "filename = \"student_sub\"+str(i)+\".csv\"\n",
    "dfo_tosend.to_csv(filename, sep=',')\n",
    "files = {'file': (filename, open(filename, 'rb'))}\n",
    "rsub = requests.post(url, files=files, auth=HTTPBasicAuth('mohamed.khanafer', 'mfalonso123'))\n",
    "resp_str = str(rsub.text)\n",
    "resp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is further increased, proving that the approach contributes to the model. I try now the same with Random Forrest.\n",
    "\n",
    "#### Experience 2: doubling the fraudulant cases + hyper parameter tunning for Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KS2 = 0.393767502472; GINI = 0.525962578212; GRADE = 9.331'"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only1 = df[df['ob_target']==1]\n",
    "result = df.append(only1)\n",
    "#result = result.append(only1)\n",
    "\n",
    "X = result[in_model]\n",
    "y = result[output_var]\n",
    "Xo = dfo[in_model] ##x out of time\n",
    "rf = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, \n",
    "                                              class_weight='balanced_subsample',\n",
    "                                              criterion='gini', max_depth=None,\n",
    "                                              max_features='auto',\n",
    "                                              max_leaf_nodes=None,\n",
    "                                              max_samples=None,\n",
    "                                              min_impurity_decrease=0.0,\n",
    "                                              min_impurity_split=None,\n",
    "                                              min_samples_leaf=1,\n",
    "                                              min_samples_split=2,\n",
    "                                              min_weight_fraction_leaf=0.0,\n",
    "                                              n_estimators=2015, n_jobs=-1,\n",
    "                                              oob_score=False,\n",
    "                                              random_state=5, verbose=0,\n",
    "                                              warm_start=False)\n",
    "\n",
    "pred_dev = rf.fit(X, y).predict_proba(X)[:,1]\n",
    "pred_oot  = rf.fit(X, y).predict_proba(Xo)[:,1]\n",
    "dfo['pred'] = pred_oot\n",
    "dfo_tosend = dfo[list(['id','pred'])]\n",
    "i=1\n",
    "filename = \"student_sub\"+str(i)+\".csv\"\n",
    "dfo_tosend.to_csv(filename, sep=',')\n",
    "files = {'file': (filename, open(filename, 'rb'))}\n",
    "rsub = requests.post(url, files=files, auth=HTTPBasicAuth('mohamed.khanafer', 'mfalonso123'))\n",
    "resp_str = str(rsub.text)\n",
    "resp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Conclusions on the weights rebalancing\n",
    "As seen again here, the changing of the weights of fraudulant cases in the data does improve the performance of the model, which should be included as well for the final model. The best approach here seemed to simply manually duplicate the fraudulant cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.  Feature Importance and Feature Selection \n",
    "We can now try filtering and selecting some features by looking at the feature importance when it comes to develop our model. I here look at various approaches to do so before making a decision.\n",
    "\n",
    "## 6.1 Features' Importance according to Scikit-Learn\n",
    "I here used the great [article](https://towardsdatascience.com/explaining-feature-importance-by-example-of-a-random-forest-d9166011959e) by Eryk Lewinson in which he illustrates the various approaches to understanding feature importance. I first start by using Scikit-Learn's default options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I define a function for creating a dataframe of features\n",
    "def imp_df(column_names, importances):\n",
    "    df = pd.DataFrame({'Feature': column_names,\n",
    "                       'Feature_importance': importances}) \\\n",
    "           .sort_values('Feature_importance', ascending = False) \\\n",
    "           .reset_index(drop = True)\n",
    "    return df\n",
    "\n",
    "# I define a function for plotting thos importance\n",
    "def var_imp_plot(imp_df, title):\n",
    "    imp_df.columns = ['Feature', 'Feature_importance']\n",
    "    plt.figure(figsize=(10,14))\n",
    "    sns.barplot(x = 'Feature_importance', y = 'Feature', data = imp_df, orient = 'h', color = 'royalblue') \\\n",
    "       .set_title(title, fontsize = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x1008 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_imp = imp_df(X.columns, rf.feature_importances_)\n",
    "var_imp_plot(base_imp, \"Scikit-Learn's default Feature Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the variables that seem the most important are the continuous ones as well as the categorical ones. But as highlighted by the above mentionned article, one of the disadvantage of this approach is is is somewhat biased, inflating the importance of continuous features or categorical variables. \n",
    "I nonetheless try running a model with the selected features only to try seeing if a higher score can be achieved. I take a treshold of the values higher than a 0.007 of importance and run a model without hyperparameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of variables selected above the 0.007 treshold are: 54 columns.\n"
     ]
    }
   ],
   "source": [
    "selected_features = base_imp['Feature'][base_imp['Feature_importance']>0.007].to_list()\n",
    "print(\"The number of variables selected above the 0.007 treshold are: \" + str(len(selected_features)) + \" columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the base Random Forrest I get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KS2 = 0.315477730848; GINI = 0.446903390974; GRADE = 7.476'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[selected_features]\n",
    "y = df[output_var]\n",
    "Xo = dfo[selected_features] \n",
    "rf = RandomForestClassifier(random_state=5)\n",
    "pred_dev = rf.fit(X, y).predict_proba(X)[:,1]\n",
    "pred_oot  = rf.fit(X, y).predict_proba(Xo)[:,1]\n",
    "dfo['pred'] = pred_oot\n",
    "dfo_tosend = dfo[list(['id','pred'])]\n",
    "i=1\n",
    "filename = \"student_sub\"+str(i)+\".csv\"\n",
    "dfo_tosend.to_csv(filename, sep=',')\n",
    "files = {'file': (filename, open(filename, 'rb'))}\n",
    "rsub = requests.post(url, files=files, auth=HTTPBasicAuth('mohamed.khanafer', 'mfalonso123'))\n",
    "resp_str = str(rsub.text)\n",
    "resp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With fewer variables, the performance seems poorer than when all the variables are included (8.549 grade). I try running the model with the hyperparameter optimized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KS2 = 0.369567879555; GINI = 0.513550263866; GRADE = 8.758'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only1 = df[df['ob_target']==1]\n",
    "result = df.append(only1)\n",
    "#result = result.append(only1)\n",
    "X = result[selected_features]\n",
    "y = result[output_var]\n",
    "Xo = dfo[selected_features] ##x out of time\n",
    "rf = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced_subsample', criterion='gini', max_depth=None,\n",
    "                                              max_features='auto', max_leaf_nodes=None, max_samples=None,min_impurity_decrease=0.0,min_impurity_split=None,min_samples_leaf=1,min_samples_split=2,\n",
    "                                              min_weight_fraction_leaf=0.0,n_estimators=2015, n_jobs=-1,oob_score=False,random_state=5, verbose=0,warm_start=False)\n",
    "pred_dev = rf.fit(X, y).predict_proba(X)[:,1]\n",
    "pred_oot  = rf.fit(X, y).predict_proba(Xo)[:,1]\n",
    "dfo['pred'] = pred_oot\n",
    "dfo_tosend = dfo[list(['id','pred'])]\n",
    "i=1\n",
    "filename = \"student_sub\"+str(i)+\".csv\"\n",
    "dfo_tosend.to_csv(filename, sep=',')\n",
    "files = {'file': (filename, open(filename, 'rb'))}\n",
    "rsub = requests.post(url, files=files, auth=HTTPBasicAuth('mohamed.khanafer', 'mfalonso123'))\n",
    "resp_str = str(rsub.text)\n",
    "resp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the some hyperparameter tunning, the model seems to give a somewhat decent result. With almost half the variables, the scores achieved are acceptable, which I believe shows that a proper approach to feature selection could lead to an increase in the model performance. I thus explore further ways of selecting features.\n",
    "\n",
    "## 6.2 Feature Importance by permutation\n",
    "This approach to feature selection measures directly the importance by assessing how random re-shuffling of each predictor in the data will influence the model performance.\n",
    "It first trains the model and saves the score; it then re-shuffles the values from one variable in the studied dataset, pass the dataset to the model again to get the predictions and compute the gini score for this modified dataset. The feature importance will then be the difference between the benchmark score and the one from the modified  dataset. And this will be repeated for all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[in_model]\n",
    "y = df[output_var]\n",
    "rf = RandomForestClassifier(random_state=5)\n",
    "\n",
    "def gini_score(rf, X, y):\n",
    "    return 2*roc_auc_score(y, rf.predict(X))\n",
    "\n",
    "perm_imp_rfpimp = permutation_importances(rf, X, y, gini_score)\n",
    "perm_imp_rfpimp.reset_index(drop = False, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x1008 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "var_imp_plot(perm_imp_rfpimp, 'Permutation feature importance (rfpimp)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of variables selected above the 0.00 treshold are: 31 columns.\n"
     ]
    }
   ],
   "source": [
    "selected_features2 = perm_imp_rfpimp['Feature'][perm_imp_rfpimp['Feature_importance']>0].to_list()\n",
    "print(\"The number of variables selected above the 0.00 treshold are: \" + str(len(selected_features2)) + \" columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KS2 = 0.354564275086; GINI = 0.476710736698; GRADE = 8.402'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[selected_features2]\n",
    "y = df[output_var]\n",
    "Xo = dfo[selected_features2] \n",
    "rf = RandomForestClassifier(random_state=5)\n",
    "pred_dev = rf.fit(X, y).predict_proba(X)[:,1]\n",
    "pred_oot  = rf.fit(X, y).predict_proba(Xo)[:,1]\n",
    "dfo['pred'] = pred_oot\n",
    "dfo_tosend = dfo[list(['id','pred'])]\n",
    "i=1\n",
    "filename = \"student_sub\"+str(i)+\".csv\"\n",
    "dfo_tosend.to_csv(filename, sep=',')\n",
    "files = {'file': (filename, open(filename, 'rb'))}\n",
    "rsub = requests.post(url, files=files, auth=HTTPBasicAuth('mohamed.khanafer', 'mfalonso123'))\n",
    "resp_str = str(rsub.text)\n",
    "resp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With only 29 columns selected, the performance is almost the same as the base model, proving once more that many variables are less significant for the model. I rerun the model with some optimized parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KS2 = 0.363775312156; GINI = 0.506221175796; GRADE = 8.62'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[selected_features2]\n",
    "y = df[output_var]\n",
    "Xo = dfo[selected_features2] \n",
    "rf = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced_subsample', criterion='gini', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None,min_impurity_decrease=0.0,min_impurity_split=None,min_samples_leaf=1,min_samples_split=2,\n",
    "                                              min_weight_fraction_leaf=0.0,n_estimators=2015, n_jobs=-1,oob_score=False,random_state=5, verbose=0,warm_start=False)\n",
    "pred_dev = rf.fit(X, y).predict_proba(X)[:,1]\n",
    "pred_oot  = rf.fit(X, y).predict_proba(Xo)[:,1]\n",
    "dfo['pred'] = pred_oot\n",
    "dfo_tosend = dfo[list(['id','pred'])]\n",
    "i=1\n",
    "filename = \"student_sub\"+str(i)+\".csv\"\n",
    "dfo_tosend.to_csv(filename, sep=',')\n",
    "files = {'file': (filename, open(filename, 'rb'))}\n",
    "rsub = requests.post(url, files=files, auth=HTTPBasicAuth('mohamed.khanafer', 'mfalonso123'))\n",
    "resp_str = str(rsub.text)\n",
    "resp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an increased performance with the tweak in parameters but not that significant, probably a better selection of variables could lead to better results.\n",
    "\n",
    "## 6.3 Feature Selection based on the PSI\n",
    "As mentionned in the [Statistics for Machine Learning](https://books.google.com.lb/books?id=C-dDDwAAQBAJ&pg=PA94&lpg=PA94&dq=population+stability+index+(PSI):+this+is+the+metric+used+to+check+that+drift&source=bl&ots=j1br-tuY-q&sig=ACfU3U3lxkf0DwE9WK_8IefldeOjRf3m_Q&hl=fr&sa=X&ved=2ahUKEwictYbC8srpAhV-BGMBHfh2B2gQ6AEwB3oECAoQAQ#v=onepage&q=population%20stability%20index%20(PSI)%3A%20this%20is%20the%20metric%20used%20to%20check%20that%20drift&f=false) book, the Population Stability Index is a metric used to check that the drift/change in the current population on which the model is used will be the same as the population in the deevelopment time. And generally, from the value, we could infer that:\n",
    "- A PSI lower than 0.1 indicates no change between the two;\n",
    "- A PSI between 0.1 and 0.25 means that some change has taken place and should call for attention but could still be used in the model;\n",
    "- A PSI larger than 0.25 means that there is an important shift in the score distribution of the current population as compared to the development time one.\n",
    "\n",
    "I used the code provided by Professor Manoel Gadi to find this PSI and potentially use it for a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The variables abovee the designed treshold of 0.25 are:\n",
      "\n",
      "psi for  id  =  0.7945807026518255\n",
      "psi for  ib_var_17  =  0.257310139829252\n",
      "psi for  ib_var_19  =  0.28185402261317133\n",
      "psi for  icn_var_22  =  0.4399049976933703\n",
      "psi for  ico_var_25  =  0.40433978511173346\n",
      "psi for  ico_var_26  =  0.7263983488673571\n",
      "psi for  ico_var_27  =  0.6362428996544015\n",
      "psi for  ico_var_28  =  0.3673456004522531\n",
      "psi for  ico_var_29  =  0.3918503438634435\n",
      "psi for  ico_var_30  =  0.35484735116851723\n",
      "psi for  ico_var_31  =  0.35918560034609304\n",
      "psi for  ico_var_32  =  0.42548068581088055\n",
      "psi for  ico_var_34  =  0.4671779717420689\n",
      "psi for  ico_var_35  =  0.39920177971399773\n",
      "psi for  ico_var_36  =  0.3032713332068569\n",
      "psi for  ico_var_43  =  0.4151434290827771\n",
      "psi for  ico_var_45  =  0.3504993546567625\n",
      "psi for  ico_var_46  =  0.637872598014254\n",
      "psi for  ico_var_49  =  0.33572516504034566\n",
      "psi for  ico_var_50  =  0.38322342755645294\n",
      "psi for  ico_var_51  =  0.300032069033377\n",
      "psi for  ico_var_53  =  0.3151581581534549\n",
      "psi for  ico_var_55  =  0.32297222351102445\n",
      "psi for  ico_var_57  =  0.2733043065031519\n",
      "psi for  ico_var_59  =  0.30512769640071885\n",
      "psi for  ico_var_60  =  0.36590864354772584\n",
      "psi for  ico_var_61  =  0.3502482239175809\n",
      "psi for  ico_var_63  =  0.26430595948620067\n",
      "psi for  ico_var_64  =  0.35225723390323593\n",
      "psi for  if_var_66  =  0.728247109921005\n",
      "psi for  if_var_73  =  0.2589391362233324\n",
      "psi for  if_var_77  =  0.27874992778664764\n",
      "psi for  if_var_79  =  0.31892296328963465\n",
      "psi for  if_var_80  =  0.26300499304105895\n"
     ]
    }
   ],
   "source": [
    "def psi(bench, comp, group):\n",
    "    ben_len=len(bench);\n",
    "    comp_len=len(comp);\n",
    "    bench.sort();\n",
    "    comp.sort();\n",
    "    psi_cut=[];\n",
    "    n=int(math.floor(ben_len/group));\n",
    "    for i in range(1,group):\n",
    "        lowercut=bench[(i-1)*n+1];\n",
    "        if i!=group:\n",
    "            uppercut=bench[(i*n)];\n",
    "            ben_cnt=n;\n",
    "        else:\n",
    "            uppercut=bench[-1];\n",
    "            ben_cnt=ben_len-group*(n-1)\n",
    "        comp_cnt = len([i for i in comp if i > lowercut and i<=uppercut]);\n",
    "        ben_pct=(ben_cnt+0.0)/ben_len;\n",
    "        comp_pct=(comp_cnt+0.0)/comp_len;\n",
    "        if comp_pct > 0.0:\n",
    "            psi_cut.append((ben_pct-comp_pct)*math.log(ben_pct/(comp_pct)));\n",
    "        else:\n",
    "            psi_cut.append(0);\n",
    "    psi=sum(psi_cut);\n",
    "    return psi;\n",
    "\n",
    "list_inputs = list()\n",
    "\n",
    "for var_name in df.columns:\n",
    "    if re.search('^i',var_name):\n",
    "        list_inputs.append(var_name)\n",
    "        \n",
    "print(\"The variables abovee the designed treshold of 0.25 are:\\n\")\n",
    "\n",
    "for var_name in list_inputs:\n",
    "    psi_value=psi(bench=list(df[var_name]),comp=list(dfo[var_name]),group=max(2,min((len(df[var_name].unique())),10)));\n",
    "    if psi_value>0.25:\n",
    "        print (\"psi for \", var_name, \" = \", psi_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I runa model without those variables to see if a significant change takes place or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KS2 = 0.286507962181; GINI = 0.410995018438; GRADE = 6.789'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_model = ['ib_var_1', 'ib_var_2', 'ib_var_3', 'ib_var_4', 'ib_var_5', 'ib_var_6', 'ib_var_7', 'ib_var_8', 'ib_var_9', 'ib_var_10','ib_var_11', 'ib_var_12', 'ib_var_13', 'ib_var_14', 'ib_var_15','ib_var_16', 'ib_var_18', 'ib_var_20','ib_var_21', 'icn_var_23', 'icn_var_24', 'ico_var_33', 'ico_var_37', 'ico_var_38', 'ico_var_39', 'ico_var_40','ico_var_41', 'ico_var_42', 'ico_var_44', 'ico_var_47', 'ico_var_48', \n",
    "        'ico_var_52', 'ico_var_54','ico_var_56', 'ico_var_58', 'ico_var_62', 'if_var_65', 'if_var_67', 'if_var_68', 'if_var_69', 'if_var_70','if_var_71', 'if_var_72', 'if_var_74', 'if_var_75','if_var_76', 'if_var_78','if_var_81']\n",
    "\n",
    "X = df[in_model]\n",
    "y = df[output_var]\n",
    "Xo = dfo[in_model] \n",
    "rf = RandomForestClassifier(random_state=5)\n",
    "pred_dev = rf.fit(X, y).predict_proba(X)[:,1]\n",
    "pred_oot  = rf.fit(X, y).predict_proba(Xo)[:,1]\n",
    "dfo['pred'] = pred_oot\n",
    "dfo_tosend = dfo[list(['id','pred'])]\n",
    "i=1\n",
    "filename = \"student_sub\"+str(i)+\".csv\"\n",
    "dfo_tosend.to_csv(filename, sep=',')\n",
    "files = {'file': (filename, open(filename, 'rb'))}\n",
    "rsub = requests.post(url, files=files, auth=HTTPBasicAuth('mohamed.khanafer', 'mfalonso123'))\n",
    "resp_str = str(rsub.text)\n",
    "resp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that our dataset is very special, the consideration of the PSI may not be useful in this case with a significant drop in the scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Dealing with the NAs\n",
    "Up until now, I had been dealing with the NAs by replacing them by 0. I now consider a different approach using [sklearn's imputers](http://scikit-learn.org/0.18/auto_examples/missing_values.html):\n",
    "\n",
    "## 7.1 Replacing NAs with the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KS2 = 0.349098651559; GINI = 0.496392064622; GRADE = 8.272'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Development sample\n",
    "df = pd.read_csv(\"dev.csv\") \n",
    "dfo = pd.read_csv(\"oot0.csv\")\n",
    "\n",
    "X = df[in_model]\n",
    "y = df[output_var]\n",
    "Xo = dfo[in_model] ##x out of time\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer = imputer.fit(Xo) \n",
    "Xo = imputer.transform(Xo)\n",
    "\n",
    "rf = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced_subsample', criterion='gini', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None,min_impurity_decrease=0.0,min_impurity_split=None,min_samples_leaf=1,min_samples_split=2,\n",
    "                                              min_weight_fraction_leaf=0.0,n_estimators=2015, n_jobs=-1,oob_score=False,random_state=5, verbose=0,warm_start=False)\n",
    "\n",
    "pred_dev = rf.fit(X, y).predict_proba(X)[:,1]\n",
    "pred_oot  = rf.fit(X, y).predict_proba(Xo)[:,1]\n",
    "dfo['pred'] = pred_oot\n",
    "dfo_tosend = dfo[list(['id','pred'])]\n",
    "i=1\n",
    "filename = \"student_sub\"+str(i)+\".csv\"\n",
    "dfo_tosend.to_csv(filename, sep=',')\n",
    "files = {'file': (filename, open(filename, 'rb'))}\n",
    "rsub = requests.post(url, files=files, auth=HTTPBasicAuth('mohamed.khanafer', 'mfalonso123'))\n",
    "resp_str = str(rsub.text)\n",
    "resp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Replacing NAs with the median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KS2 = 0.348392776273; GINI = 0.495559108679; GRADE = 8.256'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Development sample\n",
    "df = pd.read_csv(\"dev.csv\") \n",
    "dfo = pd.read_csv(\"oot0.csv\")\n",
    "\n",
    "X = df[in_model]\n",
    "y = df[output_var]\n",
    "Xo = dfo[in_model] ##x out of time\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "imputer = imputer.fit(Xo) \n",
    "Xo = imputer.transform(Xo)\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced_subsample', criterion='gini', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None,min_impurity_decrease=0.0,min_impurity_split=None,min_samples_leaf=1,min_samples_split=2,\n",
    "                                              min_weight_fraction_leaf=0.0,n_estimators=2015, n_jobs=-1,oob_score=False,random_state=5, verbose=0,warm_start=False)\n",
    "\n",
    "pred_dev = rf.fit(X, y).predict_proba(X)[:,1]\n",
    "pred_oot  = rf.fit(X, y).predict_proba(Xo)[:,1]\n",
    "dfo['pred'] = pred_oot\n",
    "dfo_tosend = dfo[list(['id','pred'])]\n",
    "i=1\n",
    "filename = \"student_sub\"+str(i)+\".csv\"\n",
    "dfo_tosend.to_csv(filename, sep=',')\n",
    "files = {'file': (filename, open(filename, 'rb'))}\n",
    "rsub = requests.post(url, files=files, auth=HTTPBasicAuth('mohamed.khanafer', 'mfalonso123'))\n",
    "resp_str = str(rsub.text)\n",
    "resp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Replacing NAs using the KNN Imputer\n",
    "The mean and median ways of imputing the missing values are the traditionnally used ways. I however here try the KNNImputer which uses the n_neighbors nearest neighbors to fill the NAs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KS2 = 0.349512241333; GINI = 0.496300797604; GRADE = 8.282'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"dev.csv\") \n",
    "dfo = pd.read_csv(\"oot0.csv\")\n",
    "X = df[in_model]\n",
    "y = df[output_var]\n",
    "Xo = dfo[in_model] ##x out of time\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "imputer = imputer.fit(Xo) \n",
    "Xo = imputer.transform(Xo)\n",
    "rf = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced_subsample', criterion='gini', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None,min_impurity_decrease=0.0,min_impurity_split=None,min_samples_leaf=1,min_samples_split=2,\n",
    "                                              min_weight_fraction_leaf=0.0,n_estimators=2000, n_jobs=-1,oob_score=False,random_state=5, verbose=0,warm_start=False)\n",
    "pred_dev = rf.fit(X, y).predict_proba(X)[:,1]\n",
    "pred_oot  = rf.fit(X, y).predict_proba(Xo)[:,1]\n",
    "dfo['pred'] = pred_oot\n",
    "dfo_tosend = dfo[list(['id','pred'])]\n",
    "filename = \"student_sub\"+str(1)+\".csv\"\n",
    "dfo_tosend.to_csv(filename, sep=',')\n",
    "files = {'file': (filename, open(filename, 'rb'))}\n",
    "rsub = requests.post(url, files=files, auth=HTTPBasicAuth('mohamed.khanafer', 'mfalonso123'))\n",
    "resp_str = str(rsub.text)\n",
    "resp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the number of neighbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KS2 = 0.34913330992; GINI = 0.495959990388; GRADE = 8.273'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"dev.csv\") \n",
    "dfo = pd.read_csv(\"oot0.csv\")\n",
    "X = df[in_model]\n",
    "y = df[output_var]\n",
    "Xo = dfo[in_model] ##x out of time\n",
    "imputer = KNNImputer(n_neighbors=6)\n",
    "imputer = imputer.fit(Xo) \n",
    "Xo = imputer.transform(Xo)\n",
    "rf = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced_subsample', criterion='gini', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None,min_impurity_decrease=0.0,min_impurity_split=None,min_samples_leaf=1,min_samples_split=2,\n",
    "                                              min_weight_fraction_leaf=0.0,n_estimators=2000, n_jobs=-1,oob_score=False,random_state=5, verbose=0,warm_start=False)\n",
    "pred_dev = rf.fit(X, y).predict_proba(X)[:,1]\n",
    "pred_oot  = rf.fit(X, y).predict_proba(Xo)[:,1]\n",
    "dfo['pred'] = pred_oot\n",
    "dfo_tosend = dfo[list(['id','pred'])]\n",
    "filename = \"student_sub\"+str(1)+\".csv\"\n",
    "dfo_tosend.to_csv(filename, sep=',')\n",
    "files = {'file': (filename, open(filename, 'rb'))}\n",
    "rsub = requests.post(url, files=files, auth=HTTPBasicAuth('mohamed.khanafer', 'mfalonso123'))\n",
    "resp_str = str(rsub.text)\n",
    "resp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the number of neighbors did not improve the performance of the model thus leaving 2 neighbors as the better parameter here.\n",
    "\n",
    "## 7.4 Replacing NAs manually with a probability\n",
    "\n",
    "instead of imputing the NAs with a strategy, we could impute these rows with simply the probability of 0, 1 or other numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dev.csv\") \n",
    "dfo = pd.read_csv(\"oot0.csv\")\n",
    "\n",
    "def is_NaN_value(value):\n",
    "    try:\n",
    "        if np.isnan(float(value)): #If current row is NaN, then return True\n",
    "            return True\n",
    "    except:\n",
    "        True\n",
    "    return False\n",
    "col = 'ib_var_1'\n",
    "row_not_to_predict = np.vectorize(is_NaN_value)(dfo[col])\n",
    "\n",
    "for col in ['ib_var_2', 'ib_var_3', 'ib_var_4', 'ib_var_5', 'ib_var_6', 'ib_var_7', 'ib_var_8', 'ib_var_9', 'ib_var_10','ib_var_11', 'ib_var_12', 'ib_var_13', 'ib_var_14', 'ib_var_15','ib_var_16', 'ib_var_17', 'ib_var_18', 'ib_var_19', 'ib_var_20','ib_var_21', 'icn_var_22', 'icn_var_23', 'icn_var_24', 'ico_var_25','ico_var_26', 'ico_var_27', 'ico_var_28', 'ico_var_29', 'ico_var_30','ico_var_31', 'ico_var_32', 'ico_var_33', 'ico_var_34', 'ico_var_35','ico_var_36', 'ico_var_37', 'ico_var_38', 'ico_var_39', 'ico_var_40','ico_var_41', 'ico_var_42', 'ico_var_43', 'ico_var_44', 'ico_var_45','ico_var_46', 'ico_var_47', 'ico_var_48', 'ico_var_49', 'ico_var_50','ico_var_51', 'ico_var_52', 'ico_var_53', 'ico_var_54', 'ico_var_55','ico_var_56', 'ico_var_57', 'ico_var_58', 'ico_var_59', 'ico_var_60','ico_var_61', 'ico_var_62', 'ico_var_63', 'ico_var_64', 'if_var_65','if_var_66', 'if_var_67', 'if_var_68', 'if_var_69', 'if_var_70','if_var_71', 'if_var_72', 'if_var_73', 'if_var_74', 'if_var_75','if_var_76', 'if_var_77', 'if_var_78', 'if_var_79', 'if_var_80','if_var_81']:\n",
    "    row_not_to_predict = np.vectorize(is_NaN_value)(dfo[col]) | row_not_to_predict\n",
    "\n",
    "dfo['row_not_to_predict'] = row_not_to_predict #THE ROWS WITH True THE MODEL SHOULD NOT APPLY - YOU SHOULD \"MANUALLY\" MARK THEN AS 1 or 0!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ib_var_1</th>\n",
       "      <th>ib_var_2</th>\n",
       "      <th>ib_var_3</th>\n",
       "      <th>ib_var_4</th>\n",
       "      <th>ib_var_5</th>\n",
       "      <th>ib_var_6</th>\n",
       "      <th>ib_var_7</th>\n",
       "      <th>ib_var_8</th>\n",
       "      <th>ib_var_9</th>\n",
       "      <th>ib_var_10</th>\n",
       "      <th>ib_var_11</th>\n",
       "      <th>ib_var_12</th>\n",
       "      <th>ib_var_13</th>\n",
       "      <th>ib_var_14</th>\n",
       "      <th>ib_var_15</th>\n",
       "      <th>ib_var_16</th>\n",
       "      <th>ib_var_17</th>\n",
       "      <th>ib_var_18</th>\n",
       "      <th>ib_var_19</th>\n",
       "      <th>ib_var_20</th>\n",
       "      <th>ib_var_21</th>\n",
       "      <th>icn_var_22</th>\n",
       "      <th>icn_var_23</th>\n",
       "      <th>icn_var_24</th>\n",
       "      <th>ico_var_25</th>\n",
       "      <th>ico_var_26</th>\n",
       "      <th>ico_var_27</th>\n",
       "      <th>ico_var_28</th>\n",
       "      <th>ico_var_29</th>\n",
       "      <th>ico_var_30</th>\n",
       "      <th>ico_var_31</th>\n",
       "      <th>ico_var_32</th>\n",
       "      <th>ico_var_33</th>\n",
       "      <th>ico_var_34</th>\n",
       "      <th>ico_var_35</th>\n",
       "      <th>ico_var_36</th>\n",
       "      <th>ico_var_37</th>\n",
       "      <th>ico_var_38</th>\n",
       "      <th>ico_var_39</th>\n",
       "      <th>ico_var_40</th>\n",
       "      <th>ico_var_41</th>\n",
       "      <th>ico_var_42</th>\n",
       "      <th>ico_var_43</th>\n",
       "      <th>ico_var_44</th>\n",
       "      <th>ico_var_45</th>\n",
       "      <th>ico_var_46</th>\n",
       "      <th>ico_var_47</th>\n",
       "      <th>ico_var_48</th>\n",
       "      <th>ico_var_49</th>\n",
       "      <th>ico_var_50</th>\n",
       "      <th>ico_var_51</th>\n",
       "      <th>ico_var_52</th>\n",
       "      <th>ico_var_53</th>\n",
       "      <th>ico_var_54</th>\n",
       "      <th>ico_var_55</th>\n",
       "      <th>ico_var_56</th>\n",
       "      <th>ico_var_57</th>\n",
       "      <th>ico_var_58</th>\n",
       "      <th>ico_var_59</th>\n",
       "      <th>ico_var_60</th>\n",
       "      <th>ico_var_61</th>\n",
       "      <th>ico_var_62</th>\n",
       "      <th>ico_var_63</th>\n",
       "      <th>ico_var_64</th>\n",
       "      <th>if_var_65</th>\n",
       "      <th>if_var_66</th>\n",
       "      <th>if_var_67</th>\n",
       "      <th>if_var_68</th>\n",
       "      <th>if_var_69</th>\n",
       "      <th>if_var_70</th>\n",
       "      <th>if_var_71</th>\n",
       "      <th>if_var_72</th>\n",
       "      <th>if_var_73</th>\n",
       "      <th>if_var_74</th>\n",
       "      <th>if_var_75</th>\n",
       "      <th>if_var_76</th>\n",
       "      <th>if_var_77</th>\n",
       "      <th>if_var_78</th>\n",
       "      <th>if_var_79</th>\n",
       "      <th>if_var_80</th>\n",
       "      <th>if_var_81</th>\n",
       "      <th>row_not_to_predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>100</td>\n",
       "      <td>8</td>\n",
       "      <td>7196.000000</td>\n",
       "      <td>5140.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.775</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5140.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>1028.000000</td>\n",
       "      <td>6168.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2570.0</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>205.600006</td>\n",
       "      <td>925.200012</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.775</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1028.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>100</td>\n",
       "      <td>14</td>\n",
       "      <td>616.799988</td>\n",
       "      <td>12027.599610</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.875</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4112.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>44</td>\n",
       "      <td>100</td>\n",
       "      <td>12</td>\n",
       "      <td>411.200012</td>\n",
       "      <td>10280.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5140.0</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2894</th>\n",
       "      <td>2895</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>792.900024</td>\n",
       "      <td>4405.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.625</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>32.7317</td>\n",
       "      <td>2643.0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2895</th>\n",
       "      <td>2896</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>7</td>\n",
       "      <td>881.000000</td>\n",
       "      <td>1762.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>4.4146</td>\n",
       "      <td>8810.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2900</th>\n",
       "      <td>2901</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>100</td>\n",
       "      <td>14</td>\n",
       "      <td>881.000000</td>\n",
       "      <td>8810.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>2.7317</td>\n",
       "      <td>4405.0</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2954</th>\n",
       "      <td>2955</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>881.000000</td>\n",
       "      <td>13215.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.4324</td>\n",
       "      <td>2643.0</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2961</th>\n",
       "      <td>2962</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>46</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>881.000000</td>\n",
       "      <td>26430.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.675</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.133333</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>2.8919</td>\n",
       "      <td>4405.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106 rows Ã— 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  ib_var_1  ib_var_2  ib_var_3  ib_var_4  ib_var_5  ib_var_6  \\\n",
       "0        1         0         1         0         0         1         0   \n",
       "2        3         0         0         0         0         1         1   \n",
       "8        9         0         1         0         0         1         1   \n",
       "10      11         0         1         0         1         1         1   \n",
       "11      12         0         0         0         0         1         1   \n",
       "...    ...       ...       ...       ...       ...       ...       ...   \n",
       "2894  2895         1         1         0         0         1         0   \n",
       "2895  2896         0         0         0         0         1         1   \n",
       "2900  2901         0         0         0         0         1         1   \n",
       "2954  2955         0         1         0         1         0         1   \n",
       "2961  2962         0         1         0         0         1         1   \n",
       "\n",
       "      ib_var_7  ib_var_8  ib_var_9  ib_var_10  ib_var_11  ib_var_12  \\\n",
       "0            0         0         0          0          0          1   \n",
       "2            0         0         0          0          0          1   \n",
       "8            0         0         0          0          0          1   \n",
       "10           0         0         0          0          0          1   \n",
       "11           0         0         0          0          0          1   \n",
       "...        ...       ...       ...        ...        ...        ...   \n",
       "2894         0         0         0          0          0          1   \n",
       "2895         0         0         0          0          0          1   \n",
       "2900         0         0         0          0          0          1   \n",
       "2954         0         1         0          0          1          1   \n",
       "2961         0         0         0          0          0          1   \n",
       "\n",
       "      ib_var_13  ib_var_14  ib_var_15  ib_var_16  ib_var_17  ib_var_18  \\\n",
       "0             0          1        0.0        0.0        1.0        0.0   \n",
       "2             1          0        1.0        1.0        1.0        0.0   \n",
       "8             0          1        1.0        1.0        1.0        1.0   \n",
       "10            0          1        0.0        1.0        1.0        1.0   \n",
       "11            0          1        1.0        1.0        1.0        0.0   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2894          0          1        0.0        0.0        1.0        0.0   \n",
       "2895          0          1        1.0        1.0        1.0        0.0   \n",
       "2900          0          1        1.0        1.0        1.0        0.0   \n",
       "2954          0          0        1.0        1.0        1.0        1.0   \n",
       "2961          1          0        1.0        1.0        1.0        1.0   \n",
       "\n",
       "      ib_var_19  ib_var_20  ib_var_21  icn_var_22  icn_var_23  icn_var_24  \\\n",
       "0           1.0        0.0        1.0         2.0           1         1.0   \n",
       "2           1.0        1.0        1.0         2.0           1         1.0   \n",
       "8           1.0        1.0        0.0         3.0           5         3.0   \n",
       "10          1.0        0.0        0.0         2.0           1         2.0   \n",
       "11          1.0        1.0        1.0         2.0           1         2.0   \n",
       "...         ...        ...        ...         ...         ...         ...   \n",
       "2894        1.0        0.0        1.0         2.0           1         NaN   \n",
       "2895        1.0        1.0        1.0         NaN           1         1.0   \n",
       "2900        1.0        1.0        1.0         NaN           1         2.0   \n",
       "2954        1.0        1.0        0.0         2.0           1         NaN   \n",
       "2961        1.0        1.0        0.0         2.0           1         NaN   \n",
       "\n",
       "      ico_var_25  ico_var_26  ico_var_27  ico_var_28  ico_var_29  ico_var_30  \\\n",
       "0              4         4.0           4           4           4           3   \n",
       "2              4         4.0           4           3           4           3   \n",
       "8              4         5.0           5           1           5           2   \n",
       "10             4         3.0           5           5           4           5   \n",
       "11             5         5.0           5           5           4           3   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2894           3         4.0           4           2           3           3   \n",
       "2895           4         3.0           3           3           3           3   \n",
       "2900           3         3.0           4           3           3           3   \n",
       "2954           4         4.0           4           4           4           4   \n",
       "2961           3         4.0           3           4           3           4   \n",
       "\n",
       "      ico_var_31  ico_var_32  ico_var_33  ico_var_34  ico_var_35  ico_var_36  \\\n",
       "0              4           4         NaN           0           0           0   \n",
       "2              3           4         1.0          11          23           9   \n",
       "8              4           5         7.0           7          19           7   \n",
       "10             4           5         9.0          17           9           9   \n",
       "11             5           4         5.0          17           7           5   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2894           2           4         5.0          12          12           1   \n",
       "2895           3           4         1.0           9          23           9   \n",
       "2900           3           3         1.0           7          12           9   \n",
       "2954           4           4         5.0           9          12           1   \n",
       "2961           3           3         7.0           9           9           1   \n",
       "\n",
       "      ico_var_37  ico_var_38  ico_var_39  ico_var_40  ico_var_41  ico_var_42  \\\n",
       "0            4.0           4           2           3           3           4   \n",
       "2            3.0           3           4           3           4           4   \n",
       "8            3.0           4           2           1           4           1   \n",
       "10           4.0           2           4           2           4           2   \n",
       "11           4.0           2           4           2           4           1   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2894         3.0           3           3           4           3           3   \n",
       "2895         4.0           4           3           3           3           3   \n",
       "2900         3.0           3           3           2           3           3   \n",
       "2954         2.0           2           2           2           2           2   \n",
       "2961         5.0           2           5           4           2           5   \n",
       "\n",
       "      ico_var_43  ico_var_44  ico_var_45  ico_var_46  ico_var_47  ico_var_48  \\\n",
       "0              4           4           2           4           2           4   \n",
       "2              4           5           2           4           4           4   \n",
       "8              5           5           1           5           3           1   \n",
       "10             4           2           3           5           2           4   \n",
       "11             5           4           2           4           2           1   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2894           4           5           2           5           4           4   \n",
       "2895           4           3           3           4           3           3   \n",
       "2900           3           2           3           3           3           3   \n",
       "2954           4           4           2           4           2           2   \n",
       "2961           5           4           2           4           5           5   \n",
       "\n",
       "      ico_var_49  ico_var_50  ico_var_51  ico_var_52  ico_var_53  ico_var_54  \\\n",
       "0              2           2           2           4           2           4   \n",
       "2              1           3           2           2           2           4   \n",
       "8              2           5           4           1           5           3   \n",
       "10             1           2           4           4           2           4   \n",
       "11             1           1           1           2           2           2   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2894           3           2           2           3           2           4   \n",
       "2895           3           3           2           3           2           4   \n",
       "2900           2           3           3           3           3           3   \n",
       "2954           2           2           2           2           2           2   \n",
       "2961           2           2           3           3           3           3   \n",
       "\n",
       "      ico_var_55  ico_var_56  ico_var_57  ico_var_58  ico_var_59  ico_var_60  \\\n",
       "0              4           4           2           3           2           2   \n",
       "2              3           2           2           4           2           1   \n",
       "8              1           2           1           3           3           3   \n",
       "10             1           2           2           2           2           2   \n",
       "11             2           2           2           4           2           2   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2894           3           1           3           3           3           2   \n",
       "2895           2           3           3           4           2           3   \n",
       "2900           3           3           3           3           3           3   \n",
       "2954           2           2           2           2           2           2   \n",
       "2961           3           2           2           5           3           2   \n",
       "\n",
       "      ico_var_61  ico_var_62  ico_var_63  ico_var_64  if_var_65  if_var_66  \\\n",
       "0              2           2           3           2         43        100   \n",
       "2              3           3           2           2         28        100   \n",
       "8              3           1           2           1         30        100   \n",
       "10             1           4           2           2         39        100   \n",
       "11             2           1           2           2         44        100   \n",
       "...          ...         ...         ...         ...        ...        ...   \n",
       "2894           3           3           2           2         34        100   \n",
       "2895           3           3           3           3         50        100   \n",
       "2900           3           3           3           2         37        100   \n",
       "2954           2           2           2           2         38        100   \n",
       "2961           3           3           2           2         46        100   \n",
       "\n",
       "      if_var_67    if_var_68     if_var_69  if_var_70  if_var_71  if_var_72  \\\n",
       "0             8  7196.000000   5140.000000          1          4          5   \n",
       "2             5  1028.000000   6168.000000          2          6          7   \n",
       "8             5   205.600006    925.200012          1          1          2   \n",
       "10           14   616.799988  12027.599610          1          2          3   \n",
       "11           12   411.200012  10280.000000          1          4          4   \n",
       "...         ...          ...           ...        ...        ...        ...   \n",
       "2894          4   792.900024   4405.000000          1          3          4   \n",
       "2895          7   881.000000   1762.000000          1          2          4   \n",
       "2900         14   881.000000   8810.000000          2          6          7   \n",
       "2954         10   881.000000  13215.000000         10          1          1   \n",
       "2961         10   881.000000  26430.000000          1          2          2   \n",
       "\n",
       "      if_var_73  if_var_74  if_var_75  if_var_76  if_var_77  if_var_78  \\\n",
       "0         0.775        1.0       10.0  11.000000   0.666667        NaN   \n",
       "2         0.725        0.0        8.0   6.000000   0.700000        NaN   \n",
       "8         0.775        5.0        4.0   3.000000   0.500000        NaN   \n",
       "10        0.875        4.0        4.0   6.000000   0.600000        NaN   \n",
       "11        0.900        0.0        6.0   8.000000   0.566667        NaN   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2894      0.625        1.0        8.0   4.000000   0.633333    32.7317   \n",
       "2895      0.650        0.0       11.0  10.000000   0.666667     4.4146   \n",
       "2900      0.625        0.0       13.0  17.000000   0.566667     2.7317   \n",
       "2954      0.800        0.0       19.0  19.000000   0.400000     1.4324   \n",
       "2961      0.675        2.0        3.0  10.133333   0.766667     2.8919   \n",
       "\n",
       "      if_var_79  if_var_80  if_var_81  row_not_to_predict  \n",
       "0        5140.0   0.666667          3                True  \n",
       "2        2570.0   0.766667          1                True  \n",
       "8        1028.0   0.666667          0                True  \n",
       "10       4112.0   0.666667          1                True  \n",
       "11       5140.0   0.600000          1                True  \n",
       "...         ...        ...        ...                 ...  \n",
       "2894     2643.0   0.800000          0                True  \n",
       "2895     8810.0   0.666667          2                True  \n",
       "2900     4405.0   0.566667          3                True  \n",
       "2954     2643.0   0.600000          3                True  \n",
       "2961     4405.0   0.833333          3                True  \n",
       "\n",
       "[106 rows x 83 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 20)\n",
    "dfo[dfo['row_not_to_predict']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had some difficulties implementing the code in python for doing the replacement manually. So I downloaded the predicted datasets, and changed them directly manually and uploaded them onto the website and the results were as follows:\n",
    "\n",
    "- Filling the NAs' prediction with 0: decrease of KS2 score by 0.02\n",
    "- Filling the NAs' prediction with 1: decrease of KS2 score by 0.015\n",
    "- Filling the NAs' prediction with 0.5: decrease of KS2 score by 0.022\n",
    "\n",
    "I can thus conclude that filling manually the values do not help in the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Some Preprocessing steps \n",
    "We can now try various preprocessing ideas to see if they will affect the performance of the model.\n",
    "\n",
    "## 8.1 Scaling with MinMaxScaler\n",
    "For fixing the scale of the data, I decided to use the MinMaxScaler from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dev.csv\")\n",
    "dfo = pd.read_csv(\"oot0.csv\")\n",
    "df = df.fillna(0)\n",
    "dfo = dfo.fillna(0)\n",
    "\n",
    "num = df[['if_var_65','if_var_66', 'if_var_67', 'if_var_68', 'if_var_69', 'if_var_70','if_var_71', 'if_var_72', 'if_var_73', 'if_var_74', 'if_var_75',\n",
    "       'if_var_76', 'if_var_77', 'if_var_78', 'if_var_79', 'if_var_80','if_var_81']]\n",
    "\n",
    "num2 = dfo[['if_var_65','if_var_66', 'if_var_67', 'if_var_68', 'if_var_69', 'if_var_70','if_var_71', 'if_var_72', 'if_var_73', 'if_var_74', 'if_var_75',\n",
    "            'if_var_76', 'if_var_77', 'if_var_78', 'if_var_79', 'if_var_80','if_var_81']]\n",
    "\n",
    "nonnum = df[['id', 'ib_var_1', 'ib_var_2', 'ib_var_3', 'ib_var_4', 'ib_var_5','ib_var_6', 'ib_var_7', 'ib_var_8', 'ib_var_9', 'ib_var_10','ib_var_11', 'ib_var_12', 'ib_var_13', 'ib_var_14', 'ib_var_15','ib_var_16', 'ib_var_17', 'ib_var_18', 'ib_var_19', 'ib_var_20','ib_var_21', 'icn_var_22', 'icn_var_23', 'icn_var_24', 'ico_var_25','ico_var_26', 'ico_var_27', 'ico_var_28', 'ico_var_29', 'ico_var_30','ico_var_31', 'ico_var_32', 'ico_var_33', 'ico_var_34', 'ico_var_35',\n",
    "       'ico_var_36','ob_target', 'ico_var_37', 'ico_var_38', 'ico_var_39', 'ico_var_40','ico_var_41', 'ico_var_42', 'ico_var_43', 'ico_var_44', 'ico_var_45','ico_var_46', 'ico_var_47', 'ico_var_48', 'ico_var_49', 'ico_var_50','ico_var_51', 'ico_var_52', 'ico_var_53', 'ico_var_54', 'ico_var_55','ico_var_56', 'ico_var_57', 'ico_var_58', 'ico_var_59', 'ico_var_60','ico_var_61', 'ico_var_62', 'ico_var_63', 'ico_var_64']]\n",
    "\n",
    "nonnum2 = dfo[['id', 'ib_var_1', 'ib_var_2', 'ib_var_3', 'ib_var_4', 'ib_var_5','ib_var_6', 'ib_var_7', 'ib_var_8', 'ib_var_9', 'ib_var_10','ib_var_11', 'ib_var_12', 'ib_var_13', 'ib_var_14', 'ib_var_15','ib_var_16', 'ib_var_17', 'ib_var_18', 'ib_var_19', 'ib_var_20','ib_var_21', 'icn_var_22', 'icn_var_23', 'icn_var_24', 'ico_var_25','ico_var_26', 'ico_var_27', 'ico_var_28', 'ico_var_29', 'ico_var_30','ico_var_31', 'ico_var_32', 'ico_var_33', 'ico_var_34', 'ico_var_35',\n",
    "       'ico_var_36', 'ico_var_37', 'ico_var_38', 'ico_var_39', 'ico_var_40','ico_var_41', 'ico_var_42', 'ico_var_43', 'ico_var_44', 'ico_var_45','ico_var_46', 'ico_var_47', 'ico_var_48', 'ico_var_49', 'ico_var_50','ico_var_51', 'ico_var_52', 'ico_var_53', 'ico_var_54', 'ico_var_55','ico_var_56', 'ico_var_57', 'ico_var_58', 'ico_var_59', 'ico_var_60','ico_var_61', 'ico_var_62', 'ico_var_63', 'ico_var_64']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>if_var_65</th>\n",
       "      <th>if_var_66</th>\n",
       "      <th>if_var_67</th>\n",
       "      <th>if_var_68</th>\n",
       "      <th>if_var_69</th>\n",
       "      <th>if_var_70</th>\n",
       "      <th>if_var_71</th>\n",
       "      <th>if_var_72</th>\n",
       "      <th>if_var_73</th>\n",
       "      <th>if_var_74</th>\n",
       "      <th>if_var_75</th>\n",
       "      <th>if_var_76</th>\n",
       "      <th>if_var_77</th>\n",
       "      <th>if_var_78</th>\n",
       "      <th>if_var_79</th>\n",
       "      <th>if_var_80</th>\n",
       "      <th>if_var_81</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.238095</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.208479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.206399</td>\n",
       "      <td>0.335946</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.309524</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.086352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.166191</td>\n",
       "      <td>0.167872</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.690476</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.208479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.243926</td>\n",
       "      <td>0.335946</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.214286</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.119118</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.175306</td>\n",
       "      <td>0.067028</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.380952</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.044650</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.121160</td>\n",
       "      <td>0.335946</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   if_var_65  if_var_66  if_var_67  if_var_68  if_var_69  if_var_70  \\\n",
       "0   0.238095        1.0      0.150   0.523810   0.208479   0.000000   \n",
       "1   0.309524        1.0      0.125   0.083333   0.086352   0.000000   \n",
       "2   0.690476        1.0      0.250   0.107143   0.208479   0.000000   \n",
       "3   0.214286        1.0      0.250   0.464286   0.119118   0.071429   \n",
       "4   0.380952        1.0      0.375   0.107143   0.044650   0.000000   \n",
       "\n",
       "   if_var_71  if_var_72  if_var_73  if_var_74  if_var_75  if_var_76  \\\n",
       "0   0.384615   0.357143   0.619048   0.000000   0.277778   0.263158   \n",
       "1   0.153846   0.071429   0.857143   0.454545   0.388889   0.263158   \n",
       "2   0.307692   0.142857   0.619048   0.272727   0.500000   0.315789   \n",
       "3   0.307692   0.357143   0.666667   0.454545   0.222222   0.315789   \n",
       "4   0.000000   0.142857   0.619048   0.000000   0.555556   0.263158   \n",
       "\n",
       "   if_var_77  if_var_78  if_var_79  if_var_80  if_var_81  \n",
       "0   0.375000   0.206399   0.335946   0.666667        0.2  \n",
       "1   0.250000   0.166191   0.167872   0.571429        0.8  \n",
       "2   0.625000   0.243926   0.335946   0.523810        0.4  \n",
       "3   0.291667   0.175306   0.067028   0.666667        0.6  \n",
       "4   0.625000   0.121160   0.335946   0.619048        0.6  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I assign the minmaxscaler from the library:\n",
    "mm_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "#I apply it to the dataframe:\n",
    "num_scaled = mm_scaler.fit_transform(num)\n",
    "num_scaled2 = mm_scaler.fit_transform(num2)\n",
    "\n",
    "# I take the names from the dataframe containing the numerical variables:\n",
    "col_names = list(num.columns)\n",
    "col_names2 = list(num2.columns)\n",
    "\n",
    "#I reassign the names to the columns\n",
    "num_scaled = pd.DataFrame(num_scaled, columns=col_names)\n",
    "num_scaled2 = pd.DataFrame(num_scaled2, columns=col_names2)\n",
    "\n",
    "#we can see the if the values are scaled:\n",
    "num_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>if_var_65</th>\n",
       "      <th>if_var_66</th>\n",
       "      <th>if_var_67</th>\n",
       "      <th>if_var_68</th>\n",
       "      <th>if_var_69</th>\n",
       "      <th>if_var_70</th>\n",
       "      <th>if_var_71</th>\n",
       "      <th>if_var_72</th>\n",
       "      <th>if_var_73</th>\n",
       "      <th>if_var_74</th>\n",
       "      <th>if_var_75</th>\n",
       "      <th>if_var_76</th>\n",
       "      <th>if_var_77</th>\n",
       "      <th>if_var_78</th>\n",
       "      <th>if_var_79</th>\n",
       "      <th>if_var_80</th>\n",
       "      <th>if_var_81</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.396825</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.821749</td>\n",
       "      <td>0.148932</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.335946</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.206349</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.049325</td>\n",
       "      <td>0.148932</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.283143</td>\n",
       "      <td>0.335946</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.158730</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.108743</td>\n",
       "      <td>0.178718</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.167872</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.037442</td>\n",
       "      <td>0.074466</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.340950</td>\n",
       "      <td>0.134257</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.253968</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.061209</td>\n",
       "      <td>0.148932</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.391264</td>\n",
       "      <td>0.309054</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   if_var_65  if_var_66  if_var_67  if_var_68  if_var_69  if_var_70  \\\n",
       "0   0.396825        1.0      0.200   0.821749   0.148932   0.000000   \n",
       "1   0.206349        1.0      0.150   0.049325   0.148932   0.000000   \n",
       "2   0.158730        1.0      0.125   0.108743   0.178718   0.020408   \n",
       "3   0.142857        1.0      0.125   0.037442   0.074466   0.000000   \n",
       "4   0.253968        1.0      0.150   0.061209   0.148932   0.040816   \n",
       "\n",
       "   if_var_71  if_var_72  if_var_73  if_var_74  if_var_75  if_var_76  \\\n",
       "0   0.230769     0.2500   0.653846   0.066667   0.526316   0.578947   \n",
       "1   0.000000     0.3125   0.615385   0.000000   0.421053   0.526316   \n",
       "2   0.384615     0.3750   0.576923   0.000000   0.421053   0.315789   \n",
       "3   0.153846     0.1875   0.884615   0.133333   0.368421   0.263158   \n",
       "4   0.000000     0.2500   0.884615   0.000000   0.526316   0.368421   \n",
       "\n",
       "   if_var_77  if_var_78  if_var_79  if_var_80  if_var_81  \n",
       "0   0.583333   0.000000   0.335946   0.523810        0.6  \n",
       "1   0.500000   0.283143   0.335946   0.619048        0.0  \n",
       "2   0.625000   0.000000   0.167872   0.666667        0.2  \n",
       "3   0.458333   0.340950   0.134257   0.761905        0.0  \n",
       "4   0.166667   0.391264   0.309054   0.476190        0.4  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_scaled2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Fixing Skewness\n",
    "Looking at the features, many of the numerical continuous ones look skewed, fixing their skewness could maybe lead to improvement of the model. I here use a function implemented from this [github repository](https://github.com/renero/dataset/blob/master/dataset/dataset.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skewed_features(df, threshold=0.75, fix=False, return_series=True):\n",
    "        feature_skew = df.apply(\n",
    "            lambda x: skew(x)).sort_values(ascending=False)\n",
    "        if fix is True:\n",
    "            high_skew = feature_skew[feature_skew > threshold]\n",
    "            skew_index = high_skew.index\n",
    "            for feature in skew_index:\n",
    "                df[feature] = boxcox1p(\n",
    "                    df[feature], boxcox_normmax(df[feature] + 1))\n",
    "            feature_skew = df.apply(\n",
    "            lambda x: skew(x)).sort_values(ascending=False)\n",
    "        if return_series is True:\n",
    "            return feature_skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "if_var_70     6.277899\n",
       "if_var_74     2.955362\n",
       "if_var_69     1.904821\n",
       "if_var_68     1.697987\n",
       "if_var_79     1.666245\n",
       "if_var_67     1.603695\n",
       "if_var_78     1.555615\n",
       "if_var_76     0.992654\n",
       "if_var_71     0.701300\n",
       "if_var_75     0.673928\n",
       "if_var_72     0.570464\n",
       "if_var_65     0.338627\n",
       "if_var_80     0.202374\n",
       "if_var_81     0.016203\n",
       "if_var_73    -0.190267\n",
       "if_var_77    -0.219504\n",
       "if_var_66   -22.469839\n",
       "dtype: float64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the skewness of the development data:\n",
    "skewed_features(num, fix= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "if_var_70    18.084994\n",
       "if_var_74     2.917955\n",
       "if_var_69     1.839562\n",
       "if_var_68     1.832461\n",
       "if_var_79     1.826398\n",
       "if_var_78     1.672071\n",
       "if_var_67     1.582539\n",
       "if_var_76     0.876468\n",
       "if_var_72     0.632438\n",
       "if_var_75     0.622399\n",
       "if_var_71     0.583520\n",
       "if_var_65     0.373355\n",
       "if_var_80     0.329321\n",
       "if_var_81     0.029467\n",
       "if_var_77    -0.141047\n",
       "if_var_73    -0.180814\n",
       "if_var_66   -20.473944\n",
       "dtype: float64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the skewness of the oot data:\n",
    "skewed_features(num2, fix= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing the skewness\n",
    "skewed_features(num, fix= True)\n",
    "skewed_features(num2, fix= True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KS2 = 0.341442619618; GINI = 0.487809499164; GRADE = 8.091'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = num.join(nonnum)\n",
    "dfo1 = num2.join(nonnum2)\n",
    "X = df1[in_model]\n",
    "y = df1[output_var]\n",
    "Xo = dfo1[in_model] ##x out of time\n",
    "rf = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced_subsample',criterion='gini', max_depth=None,\n",
    "                                              max_features='auto',max_leaf_nodes=None,max_samples=None,\n",
    "                                              min_impurity_decrease=0.0,min_impurity_split=None,\n",
    "                                              min_samples_leaf=1,min_samples_split=2,min_weight_fraction_leaf=0.0,\n",
    "                                              n_estimators=1200, n_jobs=-1,oob_score=False,random_state=5, verbose=0,warm_start=False)\n",
    "\n",
    "pred_dev = rf.fit(X, y).predict_proba(X)[:,1]\n",
    "pred_oot  = rf.fit(X, y).predict_proba(Xo)[:,1]\n",
    "dfo['pred'] = pred_oot\n",
    "dfo_tosend = dfo[list(['id','pred'])]\n",
    "i=1\n",
    "filename = \"student_sub\"+str(i)+\".csv\"\n",
    "dfo_tosend.to_csv(filename, sep=',')\n",
    "files = {'file': (filename, open(filename, 'rb'))}\n",
    "rsub = requests.post(url, files=files, auth=HTTPBasicAuth('mohamed.khanafer', 'mfalonso123'))\n",
    "resp_str = str(rsub.text)\n",
    "resp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, the fixing of skewness and scaling did not help improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Binning the continuous variables\n",
    "\n",
    "I have tried binning the continuous variables into equal width bins after scaling but this did not improve the model performance, leading to the abandonment of the idea.\n",
    "\n",
    "## 8.4 Dummy encoding the categorical variables \n",
    "\n",
    "I have tried as well dummy encoding the categorical variables with no success as well and thus did not pursue the idea.\n",
    "\n",
    "# 9. Feature Creation \n",
    "I have tried generating some new features with the polynomial features function as seen below:\n",
    "\n",
    "## 9.1 Polynomial Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KS2 = 0.365498987976; GINI = 0.510779905544; GRADE = 8.661'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[in_model]\n",
    "y = df[output_var]\n",
    "Xo = dfo[in_model]\n",
    "\n",
    "# Implementing the PolynomialFeatures\n",
    "X = PolynomialFeatures(2, interaction_only=False).fit_transform(X)\n",
    "Xo = PolynomialFeatures(2, interaction_only=False).fit_transform(Xo)\n",
    "\n",
    "rf = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced_subsample',criterion='gini', max_depth=None,\n",
    "                                              max_features='auto',max_leaf_nodes=None,max_samples=None,\n",
    "                                              min_impurity_decrease=0.0,min_impurity_split=None,\n",
    "                                              min_samples_leaf=1,min_samples_split=2,min_weight_fraction_leaf=0.0,\n",
    "                                              n_estimators=1200, n_jobs=-1,oob_score=False,random_state=5, verbose=0,warm_start=False)\n",
    "\n",
    "pred_dev = rf.fit(X, y).predict_proba(X)[:,1]\n",
    "pred_oot  = rf.fit(X, y).predict_proba(Xo)[:,1]\n",
    "dfo['pred'] = pred_oot\n",
    "dfo_tosend = dfo[list(['id','pred'])]\n",
    "i=1\n",
    "filename = \"student_sub\"+str(i)+\".csv\"\n",
    "dfo_tosend.to_csv(filename, sep=',')\n",
    "files = {'file': (filename, open(filename, 'rb'))}\n",
    "rsub = requests.post(url, files=files, auth=HTTPBasicAuth('mohamed.khanafer', 'mfalonso123'))\n",
    "resp_str = str(rsub.text)\n",
    "resp_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KS2 = 0.358074011775; GINI = 0.502992171832; GRADE = 8.485'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[in_model]\n",
    "y = df[output_var]\n",
    "Xo = dfo[in_model]\n",
    "\n",
    "# Implementing the PolynomialFeatures\n",
    "X = PolynomialFeatures(3, interaction_only=False).fit_transform(X)\n",
    "Xo = PolynomialFeatures(3, interaction_only=False).fit_transform(Xo)\n",
    "\n",
    "rf = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced_subsample',criterion='gini', max_depth=None,\n",
    "                                              max_features='auto',max_leaf_nodes=None,max_samples=None,\n",
    "                                              min_impurity_decrease=0.0,min_impurity_split=None,\n",
    "                                              min_samples_leaf=1,min_samples_split=2,min_weight_fraction_leaf=0.0,\n",
    "                                              n_estimators=1200, n_jobs=-1,oob_score=False,random_state=5, verbose=0,warm_start=False)\n",
    "\n",
    "pred_dev = rf.fit(X, y).predict_proba(X)[:,1]\n",
    "pred_oot  = rf.fit(X, y).predict_proba(Xo)[:,1]\n",
    "dfo['pred'] = pred_oot\n",
    "dfo_tosend = dfo[list(['id','pred'])]\n",
    "i=1\n",
    "filename = \"student_sub\"+str(i)+\".csv\"\n",
    "dfo_tosend.to_csv(filename, sep=',')\n",
    "files = {'file': (filename, open(filename, 'rb'))}\n",
    "rsub = requests.post(url, files=files, auth=HTTPBasicAuth('mohamed.khanafer', 'mfalonso123'))\n",
    "resp_str = str(rsub.text)\n",
    "resp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial feature generations did not improve the score of the initial model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 10. Combining insights for the  Models\n",
    "\n",
    "From the insights gathered all throughout the flow, I combine them into various models that I test here before choosing the best model:\n",
    "\n",
    "## 10.1 XGBClassifier \n",
    "\n",
    "### 10.1.1 XGBC Model I\n",
    "The approaches included: ``Manual Hyper parameter tunning``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KS2 = 0.334043059548; GINI = 0.411678943428; GRADE = 7.916'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[in_model]\n",
    "y = df[output_var]\n",
    "Xo = dfo[in_model] ##x out of time\n",
    "xgbcla = XGBClassifier(booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
    "       max_depth=3, min_child_weight=1, missing=None, n_estimators=1000,\n",
    "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=5, seed=None, silent=True)\n",
    "\n",
    "pred_dev = xgbcla.fit(X, y).predict_proba(X)[:,1]\n",
    "pred_oot  = xgbcla.fit(X, y).predict_proba(Xo)[:,1]\n",
    "dfo['pred'] = pred_oot\n",
    "dfo_tosend = dfo[list(['id','pred'])]\n",
    "i=1\n",
    "filename = \"student_sub\"+str(i)+\".csv\"\n",
    "dfo_tosend.to_csv(filename, sep=',')\n",
    "files = {'file': (filename, open(filename, 'rb'))}\n",
    "rsub = requests.post(url, files=files, auth=HTTPBasicAuth('mohamed.khanafer', 'mfalonso123'))\n",
    "resp_str = str(rsub.text)\n",
    "resp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.2 XGBC Model  II\n",
    "The approaches included: ``Manual Hyper parameter tunning`` + ``Oversampling`` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KS2 = 0.331618129558; GINI = 0.41020249725; GRADE = 7.858'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[in_model]\n",
    "y = df[output_var]\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "X_over, y_over = oversample.fit_resample(X, y)\n",
    "\n",
    "X = X_over\n",
    "y = y_over\n",
    "Xo = dfo[in_model] ##x out of time\n",
    "xgbcla = XGBClassifier(booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
    "       max_depth=3, min_child_weight=1, missing=None, n_estimators=2000,\n",
    "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=5, seed=None, silent=True)\n",
    "\n",
    "pred_dev = xgbcla.fit(X, y).predict_proba(X)[:,1]\n",
    "pred_oot  = xgbcla.fit(X, y).predict_proba(Xo)[:,1]\n",
    "dfo['pred'] = pred_oot\n",
    "dfo_tosend = dfo[list(['id','pred'])]\n",
    "i=1\n",
    "filename = \"student_sub\"+str(i)+\".csv\"\n",
    "dfo_tosend.to_csv(filename, sep=',')\n",
    "files = {'file': (filename, open(filename, 'rb'))}\n",
    "rsub = requests.post(url, files=files, auth=HTTPBasicAuth('mohamed.khanafer', 'mfalonso123'))\n",
    "resp_str = str(rsub.text)\n",
    "resp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.3 XGBC Model III\n",
    "The approaches included: ``Manual Hyper parameter tunning`` + ``Manually duplicated Fraud`` + ``Manual Feature Selection``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KS2 = 0.351805469551; GINI = 0.431732271093; GRADE = 8.337'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only1 = df[df['ob_target']==1]\n",
    "result = df.append(only1)\n",
    "result = result.append(only1)\n",
    "\n",
    "features_selected = ['ib_var_1', 'ib_var_2', 'ib_var_3', 'ib_var_4', 'ib_var_5','ib_var_6', 'ib_var_7', 'ib_var_8', 'ib_var_9', 'ib_var_10','ib_var_11', 'ib_var_12', 'ib_var_13', 'ib_var_14', 'ib_var_15','ib_var_16', 'ib_var_17', 'ib_var_18', 'ib_var_19', 'ib_var_20','ib_var_21', 'icn_var_22', 'icn_var_23', 'icn_var_24', 'ico_var_25','ico_var_26', 'ico_var_27', 'ico_var_28', \n",
    "            'ico_var_29', 'ico_var_30','ico_var_31', 'ico_var_32', 'ico_var_33', 'ico_var_34', 'ico_var_35','ico_var_36', 'ico_var_37', 'ico_var_38', 'ico_var_39', 'ico_var_40','ico_var_41', 'ico_var_42', 'ico_var_43', 'ico_var_44', 'ico_var_45','ico_var_46', 'ico_var_47', 'ico_var_48', 'ico_var_49', 'ico_var_50','if_var_71', 'if_var_72', 'if_var_73', 'if_var_74', 'if_var_75','if_var_76', 'if_var_77', 'if_var_78', 'if_var_79', 'if_var_80','if_var_81']\n",
    "\n",
    "X = result[features_selected]\n",
    "y = result[output_var]\n",
    "Xo = dfo[features_selected] ##x out of time\n",
    "xgbcla = XGBClassifier(booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
    "       max_depth=3, min_child_weight=1, missing=None, n_estimators=2000,\n",
    "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=5, seed=None, silent=True)\n",
    "\n",
    "pred_dev = xgbcla.fit(X, y).predict_proba(X)[:,1]\n",
    "pred_oot  = xgbcla.fit(X, y).predict_proba(Xo)[:,1]\n",
    "dfo['pred'] = pred_oot\n",
    "dfo_tosend = dfo[list(['id','pred'])]\n",
    "i=1\n",
    "filename = \"student_sub\"+str(i)+\".csv\"\n",
    "dfo_tosend.to_csv(filename, sep=',')\n",
    "files = {'file': (filename, open(filename, 'rb'))}\n",
    "rsub = requests.post(url, files=files, auth=HTTPBasicAuth('mohamed.khanafer', 'mfalonso123'))\n",
    "resp_str = str(rsub.text)\n",
    "resp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 CatBoost Classifier \n",
    "\n",
    "### 10.2.1 CATB Model I\n",
    "The approaches included: ``Manual Hyper parameter tunning``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KS2 = 0.356642621466; GINI = 0.465246906164; GRADE = 8.451'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_model = ['ib_var_1', 'ib_var_2', 'ib_var_3', 'ib_var_4', 'ib_var_5', 'ib_var_6', 'ib_var_7', 'ib_var_8', 'ib_var_9', 'ib_var_10','ib_var_11', 'ib_var_12', 'ib_var_13', 'ib_var_14', 'ib_var_15','ib_var_16', 'ib_var_17', 'ib_var_18', 'ib_var_19', 'ib_var_20','ib_var_21', 'icn_var_22', 'icn_var_23', 'icn_var_24', 'ico_var_25','ico_var_26', 'ico_var_27', 'ico_var_28', 'ico_var_29', 'ico_var_30','ico_var_31', 'ico_var_32', 'ico_var_33', 'ico_var_34', 'ico_var_35','ico_var_36', 'ico_var_37', 'ico_var_38', 'ico_var_39', 'ico_var_40','ico_var_41', 'ico_var_42', 'ico_var_43', 'ico_var_44', 'ico_var_45','ico_var_46', 'ico_var_47', 'ico_var_48', 'ico_var_49', 'ico_var_50',\n",
    "       'ico_var_51', 'ico_var_52', 'ico_var_53', 'ico_var_54', 'ico_var_55','ico_var_56', 'ico_var_57', 'ico_var_58', 'ico_var_59', 'ico_var_60','ico_var_61', 'ico_var_62', 'ico_var_63', 'ico_var_64', 'if_var_65','if_var_66', 'if_var_67', 'if_var_68', 'if_var_69', 'if_var_70','if_var_71', 'if_var_72', 'if_var_73', 'if_var_74', 'if_var_75','if_var_76', 'if_var_77', 'if_var_78', 'if_var_79', 'if_var_80','if_var_81']\n",
    "X = df[in_model]\n",
    "y = df[output_var]\n",
    "Xo = dfo[in_model] ##x out of time\n",
    "catb = CatBoostClassifier(random_seed=5, eval_metric = 'Logloss', n_estimators = 2000)\n",
    "blockPrint()\n",
    "pred_dev = catb.fit(X, y).predict_proba(X)[:,1]\n",
    "pred_oot  = catb.fit(X, y).predict_proba(Xo)[:,1]\n",
    "dfo['pred'] = pred_oot\n",
    "dfo_tosend = dfo[list(['id','pred'])]\n",
    "i=1\n",
    "filename = \"student_sub\"+str(i)+\".csv\"\n",
    "dfo_tosend.to_csv(filename, sep=',')\n",
    "files = {'file': (filename, open(filename, 'rb'))}\n",
    "rsub = requests.post(url, files=files, auth=HTTPBasicAuth('mohamed.khanafer', 'mfalonso123'))\n",
    "resp_str = str(rsub.text)\n",
    "resp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.2 CATB Model  II\n",
    "The approaches included: ``Manual Hyper parameter tunning`` + ``Oversampling``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KS2 = 0.377830432814; GINI = 0.463065739979; GRADE = 8.953'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_model = ['ib_var_1', 'ib_var_2', 'ib_var_3', 'ib_var_4', 'ib_var_5', 'ib_var_6', 'ib_var_7', 'ib_var_8', 'ib_var_9', 'ib_var_10','ib_var_11', 'ib_var_12', 'ib_var_13', 'ib_var_14', 'ib_var_15','ib_var_16', 'ib_var_17', 'ib_var_18', 'ib_var_19', 'ib_var_20','ib_var_21', 'icn_var_22', 'icn_var_23', 'icn_var_24', 'ico_var_25','ico_var_26', 'ico_var_27', 'ico_var_28', 'ico_var_29', 'ico_var_30','ico_var_31', 'ico_var_32', 'ico_var_33', 'ico_var_34', 'ico_var_35','ico_var_36', 'ico_var_37', 'ico_var_38', 'ico_var_39', 'ico_var_40','ico_var_41', 'ico_var_42', 'ico_var_43', 'ico_var_44', 'ico_var_45','ico_var_46', 'ico_var_47', 'ico_var_48', 'ico_var_49', 'ico_var_50',\n",
    "       'ico_var_51', 'ico_var_52', 'ico_var_53', 'ico_var_54', 'ico_var_55','ico_var_56', 'ico_var_57', 'ico_var_58', 'ico_var_59', 'ico_var_60','ico_var_61', 'ico_var_62', 'ico_var_63', 'ico_var_64', 'if_var_65','if_var_66', 'if_var_67', 'if_var_68', 'if_var_69', 'if_var_70','if_var_71', 'if_var_72', 'if_var_73', 'if_var_74', 'if_var_75','if_var_76', 'if_var_77', 'if_var_78', 'if_var_79', 'if_var_80','if_var_81']\n",
    "X = df[in_model]\n",
    "y = df[output_var]\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "X_over, y_over = oversample.fit_resample(X, y)\n",
    "\n",
    "X = X_over\n",
    "y = y_over\n",
    "Xo = dfo[in_model] ##x out of time\n",
    "catb = CatBoostClassifier(random_seed=5, eval_metric = 'Logloss', n_estimators = 2000)\n",
    "blockPrint()\n",
    "pred_dev = catb.fit(X, y).predict_proba(X)[:,1]\n",
    "pred_oot  = catb.fit(X, y).predict_proba(Xo)[:,1]\n",
    "dfo['pred'] = pred_oot\n",
    "dfo_tosend = dfo[list(['id','pred'])]\n",
    "i=1\n",
    "filename = \"student_sub\"+str(i)+\".csv\"\n",
    "dfo_tosend.to_csv(filename, sep=',')\n",
    "files = {'file': (filename, open(filename, 'rb'))}\n",
    "rsub = requests.post(url, files=files, auth=HTTPBasicAuth('mohamed.khanafer', 'mfalonso123'))\n",
    "resp_str = str(rsub.text)\n",
    "resp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.3 CATB Model III\n",
    "The approaches included: ``Manual Hyper parameter tunning`` + ``Manually duplicated Fraud`` + ``Manual Feature Selection``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KS2 = 0.382429597316; GINI = 0.483287738334; GRADE = 9.062'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only1 = df[df['ob_target']==1]\n",
    "result = df.append(only1)\n",
    "result = result.append(only1)\n",
    "\n",
    "features_selected = ['ib_var_1', 'ib_var_2', 'ib_var_3', 'ib_var_4', 'ib_var_5','ib_var_6', 'ib_var_7', 'ib_var_8', 'ib_var_9', 'ib_var_10','ib_var_11', 'ib_var_12', 'ib_var_13', 'ib_var_14', 'ib_var_15','ib_var_16', 'ib_var_17', 'ib_var_18', 'ib_var_19', 'ib_var_20','ib_var_21', 'icn_var_22', 'icn_var_23', 'icn_var_24', 'ico_var_25','ico_var_26', 'ico_var_27', 'ico_var_28', \n",
    "            'ico_var_29', 'ico_var_30','ico_var_31', 'ico_var_32', 'ico_var_33', 'ico_var_34', 'ico_var_35','ico_var_36', 'ico_var_37', 'ico_var_38', 'ico_var_39', 'ico_var_40','ico_var_41', 'ico_var_42', 'ico_var_43', 'ico_var_44', 'ico_var_45','ico_var_46', 'ico_var_47', 'ico_var_48', 'ico_var_49', 'ico_var_50','if_var_71', 'if_var_72', 'if_var_73', 'if_var_74', 'if_var_75','if_var_76', 'if_var_77', 'if_var_78', 'if_var_79', 'if_var_80','if_var_81']\n",
    "\n",
    "X = result[features_selected]\n",
    "y = result[output_var]\n",
    "Xo = dfo[features_selected] ##x out of time\n",
    "catb = CatBoostClassifier(random_seed=5, eval_metric = 'Logloss', n_estimators = 2000)\n",
    "blockPrint()\n",
    "pred_dev = catb.fit(X, y).predict_proba(X)[:,1]\n",
    "pred_oot  = catb.fit(X, y).predict_proba(Xo)[:,1]\n",
    "dfo['pred'] = pred_oot\n",
    "dfo_tosend = dfo[list(['id','pred'])]\n",
    "i=1\n",
    "filename = \"student_sub\"+str(i)+\".csv\"\n",
    "dfo_tosend.to_csv(filename, sep=',')\n",
    "files = {'file': (filename, open(filename, 'rb'))}\n",
    "rsub = requests.post(url, files=files, auth=HTTPBasicAuth('mohamed.khanafer', 'mfalonso123'))\n",
    "resp_str = str(rsub.text)\n",
    "resp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 RandomForest Classifier\n",
    "\n",
    "### 10.3.1 RF Model I\n",
    "The approaches included: ``Manual Hyper parameter tunning``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KS2 = 0.375561465448; GINI = 0.527450577177; GRADE = 8.9'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_model = ['ib_var_1', 'ib_var_2', 'ib_var_3', 'ib_var_4', 'ib_var_5', 'ib_var_6', 'ib_var_7', 'ib_var_8', 'ib_var_9', 'ib_var_10','ib_var_11', 'ib_var_12', 'ib_var_13', 'ib_var_14', 'ib_var_15','ib_var_16', 'ib_var_17', 'ib_var_18', 'ib_var_19', 'ib_var_20','ib_var_21', 'icn_var_22', 'icn_var_23', 'icn_var_24', 'ico_var_25','ico_var_26', 'ico_var_27', 'ico_var_28', 'ico_var_29', 'ico_var_30','ico_var_31', 'ico_var_32', 'ico_var_33', 'ico_var_34', 'ico_var_35','ico_var_36', 'ico_var_37', 'ico_var_38', 'ico_var_39', 'ico_var_40','ico_var_41', 'ico_var_42', 'ico_var_43', 'ico_var_44', 'ico_var_45','ico_var_46', 'ico_var_47', 'ico_var_48', 'ico_var_49', 'ico_var_50',\n",
    "       'ico_var_51', 'ico_var_52', 'ico_var_53', 'ico_var_54', 'ico_var_55','ico_var_56', 'ico_var_57', 'ico_var_58', 'ico_var_59', 'ico_var_60','ico_var_61', 'ico_var_62', 'ico_var_63', 'ico_var_64', 'if_var_65','if_var_66', 'if_var_67', 'if_var_68', 'if_var_69', 'if_var_70','if_var_71', 'if_var_72', 'if_var_73', 'if_var_74', 'if_var_75','if_var_76', 'if_var_77', 'if_var_78', 'if_var_79', 'if_var_80','if_var_81']\n",
    "X = df[in_model]\n",
    "y = df[output_var]\n",
    "Xo = dfo[in_model]\n",
    "rf = RandomForestClassifier(criterion='gini', max_depth=None, min_samples_split=2,min_samples_leaf=1, \n",
    "                                              min_weight_fraction_leaf=0.0, max_features='log2', \n",
    "                                              max_leaf_nodes=None, bootstrap=True, oob_score=True,\n",
    "                                              n_jobs=1, random_state=5,\n",
    "                                              verbose=0, warm_start=False, \n",
    "                                              class_weight='balanced_subsample', n_estimators = 2000)\n",
    "fitted_model = rf.fit(X, y)\n",
    "pred_dev = rf.predict_proba(X)[:,1]\n",
    "pred_oot  = rf.predict_proba(Xo)[:,1]\n",
    "dfo['pred'] = pred_oot\n",
    "dfo_tosend = dfo[list(['id','pred'])]\n",
    "i=1\n",
    "filename = \"student_sub\"+str(i)+\".csv\"\n",
    "dfo_tosend.to_csv(filename, sep=',')\n",
    "files = {'file': (filename, open(filename, 'rb'))}\n",
    "rsub = requests.post(url, files=files, auth=HTTPBasicAuth('mohamed.khanafer', 'mfalonso123'))\n",
    "resp_str = str(rsub.text)\n",
    "resp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.2 RF Model  II\n",
    "The approaches included: ``Manual Hyper parameter tunning`` + ``Oversampling``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KS2 = 0.382174280723; GINI = 0.527942725903; GRADE = 9.056'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_model = ['ib_var_1', 'ib_var_2', 'ib_var_3', 'ib_var_4', 'ib_var_5', 'ib_var_6', 'ib_var_7', 'ib_var_8', 'ib_var_9', 'ib_var_10','ib_var_11', 'ib_var_12', 'ib_var_13', 'ib_var_14', 'ib_var_15','ib_var_16', 'ib_var_17', 'ib_var_18', 'ib_var_19', 'ib_var_20','ib_var_21', 'icn_var_22', 'icn_var_23', 'icn_var_24', 'ico_var_25','ico_var_26', 'ico_var_27', 'ico_var_28', 'ico_var_29', 'ico_var_30','ico_var_31', 'ico_var_32', 'ico_var_33', 'ico_var_34', 'ico_var_35','ico_var_36', 'ico_var_37', 'ico_var_38', 'ico_var_39', 'ico_var_40','ico_var_41', 'ico_var_42', 'ico_var_43', 'ico_var_44', 'ico_var_45','ico_var_46', 'ico_var_47', 'ico_var_48', 'ico_var_49', 'ico_var_50',\n",
    "       'ico_var_51', 'ico_var_52', 'ico_var_53', 'ico_var_54', 'ico_var_55','ico_var_56', 'ico_var_57', 'ico_var_58', 'ico_var_59', 'ico_var_60','ico_var_61', 'ico_var_62', 'ico_var_63', 'ico_var_64', 'if_var_65','if_var_66', 'if_var_67', 'if_var_68', 'if_var_69', 'if_var_70','if_var_71', 'if_var_72', 'if_var_73', 'if_var_74', 'if_var_75','if_var_76', 'if_var_77', 'if_var_78', 'if_var_79', 'if_var_80','if_var_81']\n",
    "X = df[in_model]\n",
    "y = df[output_var]\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "X_over, y_over = oversample.fit_resample(X, y)\n",
    "X = X_over\n",
    "y = y_over\n",
    "rf = RandomForestClassifier(criterion='gini', max_depth=None, min_samples_split=2,min_samples_leaf=1, \n",
    "                                              min_weight_fraction_leaf=0.0, max_features='log2', \n",
    "                                              max_leaf_nodes=None, bootstrap=True, oob_score=True,\n",
    "                                              n_jobs=1, random_state=5,\n",
    "                                              verbose=0, warm_start=False, \n",
    "                                              class_weight='balanced_subsample', n_estimators = 2000)\n",
    "fitted_model = rf.fit(X, y)\n",
    "pred_dev = rf.predict_proba(X)[:,1]\n",
    "pred_oot  = rf.predict_proba(Xo)[:,1]\n",
    "dfo['pred'] = pred_oot\n",
    "dfo_tosend = dfo[list(['id','pred'])]\n",
    "i=1\n",
    "filename = \"student_sub\"+str(i)+\".csv\"\n",
    "dfo_tosend.to_csv(filename, sep=',')\n",
    "files = {'file': (filename, open(filename, 'rb'))}\n",
    "rsub = requests.post(url, files=files, auth=HTTPBasicAuth('mohamed.khanafer', 'mfalonso123'))\n",
    "resp_str = str(rsub.text)\n",
    "resp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.3 RF Model III\n",
    "The approaches included: ``Manual Hyper parameter tunning`` + ``Manually duplicated Fraud`` + ``Manual Feature Selection``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KS2 = 0.42199904805; GINI = 0.539219401288; GRADE = 10.0'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_selected = ['ib_var_1', 'ib_var_2', 'ib_var_3', 'ib_var_4', 'ib_var_5','ib_var_6', 'ib_var_7', 'ib_var_8', 'ib_var_9', 'ib_var_10','ib_var_11', 'ib_var_12', 'ib_var_13', 'ib_var_14', 'ib_var_15','ib_var_16', 'ib_var_17', 'ib_var_18', 'ib_var_19', 'ib_var_20','ib_var_21', \n",
    "            'icn_var_22', 'icn_var_23', 'icn_var_24', 'ico_var_25','ico_var_26', 'ico_var_27', 'ico_var_28', 'ico_var_29', 'ico_var_30','ico_var_31', 'ico_var_32', 'ico_var_33', 'ico_var_34', 'ico_var_35','ico_var_36', 'ico_var_37', 'ico_var_38', 'ico_var_39', 'ico_var_40','ico_var_41', 'ico_var_42', 'ico_var_43', 'ico_var_44', 'ico_var_45','ico_var_46', 'ico_var_47', 'ico_var_48', 'ico_var_49', 'ico_var_50','if_var_71', 'if_var_72', 'if_var_73', 'if_var_74', 'if_var_75','if_var_76', 'if_var_77', 'if_var_78', 'if_var_79', 'if_var_80','if_var_81']\n",
    "\n",
    "only1 = df[df['ob_target']==1]\n",
    "result = only1.append(df)\n",
    "result = result.append(only1)\n",
    "\n",
    "X = result[features_selected]\n",
    "y = result[output_var]\n",
    "Xo = dfo[features_selected]\n",
    "\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(criterion='gini', max_depth=None, min_samples_split=2,min_samples_leaf=1, \n",
    "                                              min_weight_fraction_leaf=0.0, max_features='log2', \n",
    "                                              max_leaf_nodes=None, bootstrap=True, oob_score=True,\n",
    "                                              n_jobs=1, random_state=5,\n",
    "                                              verbose=0, warm_start=False, \n",
    "                                              class_weight='balanced_subsample', n_estimators = 2000)\n",
    "\n",
    "fitted_model = rf.fit(X, y)\n",
    "pred_dev = rf.predict_proba(X)[:,1]\n",
    "pred_oot  = rf.predict_proba(Xo)[:,1]\n",
    "\n",
    "dfo['pred'] = pred_oot\n",
    "dfo_tosend = dfo[list(['id','pred'])]\n",
    "i=1\n",
    "filename = \"student_sub\"+str(i)+\".csv\"\n",
    "dfo_tosend.to_csv(filename, sep=',')\n",
    "files = {'file': (filename, open(filename, 'rb'))}\n",
    "rsub = requests.post(url, files=files, auth=HTTPBasicAuth('mohamed.khanafer', 'mfalonso123'))\n",
    "resp_str = str(rsub.text)\n",
    "resp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 Ensemble Models\n",
    "\n",
    "### 10.4.1 RF and CATB\n",
    "VotingClassifier model with:  ``Manual Hyper parameter tunning`` + ``Manually duplicated Fraud`` + ``Manual Feature Selection``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KS2 = 0.406393543378; GINI = 0.535233689775; GRADE = 9.63'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_selected = ['ib_var_1', 'ib_var_2', 'ib_var_3', 'ib_var_4', 'ib_var_5','ib_var_6', 'ib_var_7', 'ib_var_8', 'ib_var_9', 'ib_var_10','ib_var_11', 'ib_var_12', 'ib_var_13', 'ib_var_14', 'ib_var_15','ib_var_16', 'ib_var_17', 'ib_var_18', 'ib_var_19', 'ib_var_20','ib_var_21', \n",
    "            'icn_var_22', 'icn_var_23', 'icn_var_24', 'ico_var_25','ico_var_26', 'ico_var_27', 'ico_var_28', 'ico_var_29', 'ico_var_30','ico_var_31', 'ico_var_32', 'ico_var_33', 'ico_var_34', 'ico_var_35','ico_var_36', 'ico_var_37', 'ico_var_38', 'ico_var_39', 'ico_var_40','ico_var_41', 'ico_var_42', 'ico_var_43', 'ico_var_44', 'ico_var_45','ico_var_46', 'ico_var_47', 'ico_var_48', 'ico_var_49', 'ico_var_50','if_var_71', 'if_var_72', 'if_var_73', 'if_var_74', 'if_var_75','if_var_76', 'if_var_77', 'if_var_78', 'if_var_79', 'if_var_80','if_var_81']\n",
    "\n",
    "only1 = df[df['ob_target']==1]\n",
    "result = only1.append(df)\n",
    "result = result.append(only1)\n",
    "\n",
    "X = result[features_selected]\n",
    "y = result[output_var]\n",
    "Xo = dfo[features_selected]\n",
    "\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(criterion='gini', max_depth=None, min_samples_split=2,min_samples_leaf=1, \n",
    "                                              min_weight_fraction_leaf=0.0, max_features='log2', \n",
    "                                              max_leaf_nodes=None, bootstrap=True, oob_score=True,\n",
    "                                              n_jobs=1, random_state=5,\n",
    "                                              verbose=0, warm_start=False, \n",
    "                                              class_weight='balanced_subsample', n_estimators = 2000)\n",
    "\n",
    "catb = CatBoostClassifier(random_seed=5, eval_metric = 'Logloss', n_estimators = 2000)\n",
    "\n",
    "# We create a dictionary with our models:\n",
    "estimators=[('randomF', rf), ('CatBoostClassifier', catb) ]\n",
    "#create our voting classifier, inputting our models\n",
    "ensemble = VotingClassifier(estimators, voting='soft')\n",
    "\n",
    "fitted_model = ensemble.fit(X, y)\n",
    "pred_dev = ensemble.predict_proba(X)[:,1]\n",
    "pred_oot  = ensemble.predict_proba(Xo)[:,1]\n",
    "\n",
    "dfo['pred'] = pred_oot\n",
    "dfo_tosend = dfo[list(['id','pred'])]\n",
    "i=1\n",
    "filename = \"student_sub\"+str(i)+\".csv\"\n",
    "dfo_tosend.to_csv(filename, sep=',')\n",
    "files = {'file': (filename, open(filename, 'rb'))}\n",
    "rsub = requests.post(url, files=files, auth=HTTPBasicAuth('mohamed.khanafer', 'mfalonso123'))\n",
    "resp_str = str(rsub.text)\n",
    "resp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4.2 Combination of 3 RF\n",
    "VotingClassifier model with:  ``Manual Hyper parameter tunning`` + ``Manually duplicated Fraud`` + ``Manual Feature Selection``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KS2 = 0.421189197682; GINI = 0.538129973475; GRADE = 9.981'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_selected = ['ib_var_1', 'ib_var_2', 'ib_var_3', 'ib_var_4', 'ib_var_5','ib_var_6', 'ib_var_7', 'ib_var_8', 'ib_var_9', 'ib_var_10','ib_var_11', 'ib_var_12', 'ib_var_13', 'ib_var_14', 'ib_var_15','ib_var_16', 'ib_var_17', 'ib_var_18', 'ib_var_19', 'ib_var_20','ib_var_21', \n",
    "            'icn_var_22', 'icn_var_23', 'icn_var_24', 'ico_var_25','ico_var_26', 'ico_var_27', 'ico_var_28', 'ico_var_29', 'ico_var_30','ico_var_31', 'ico_var_32', 'ico_var_33', 'ico_var_34', 'ico_var_35','ico_var_36', 'ico_var_37', 'ico_var_38', 'ico_var_39', 'ico_var_40','ico_var_41', 'ico_var_42', 'ico_var_43', 'ico_var_44', 'ico_var_45','ico_var_46', 'ico_var_47', 'ico_var_48', 'ico_var_49', 'ico_var_50','if_var_71', 'if_var_72', 'if_var_73', 'if_var_74', 'if_var_75','if_var_76', 'if_var_77', 'if_var_78', 'if_var_79', 'if_var_80','if_var_81']\n",
    "\n",
    "only1 = df[df['ob_target']==1]\n",
    "result = only1.append(df)\n",
    "result = result.append(only1)\n",
    "\n",
    "X = result[features_selected]\n",
    "y = result[output_var]\n",
    "Xo = dfo[features_selected]\n",
    "\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(criterion='gini', max_depth=None, min_samples_split=2,min_samples_leaf=1, \n",
    "                                              min_weight_fraction_leaf=0.0, max_features='log2', \n",
    "                                              max_leaf_nodes=None, bootstrap=True, oob_score=True,\n",
    "                                              n_jobs=1, random_state=5,\n",
    "                                              verbose=0, warm_start=False, \n",
    "                                              class_weight='balanced_subsample', n_estimators = 2000)\n",
    "\n",
    "rf2 = RandomForestClassifier(criterion='gini', max_depth=None, min_samples_split=2,min_samples_leaf=1, \n",
    "                                              min_weight_fraction_leaf=0.0, max_features='log2', \n",
    "                                              max_leaf_nodes=None, bootstrap=True, oob_score=True,\n",
    "                                              n_jobs=1, random_state=5,\n",
    "                                              verbose=0, warm_start=False, \n",
    "                                              class_weight='balanced_subsample', n_estimators = 2910)\n",
    "\n",
    "rf3 = RandomForestClassifier(criterion='gini', max_depth=None, min_samples_split=2,min_samples_leaf=1, \n",
    "                                              min_weight_fraction_leaf=0.0, max_features='log2', \n",
    "                                              max_leaf_nodes=None, bootstrap=True, oob_score=True,\n",
    "                                              n_jobs=1, random_state=5,\n",
    "                                              verbose=0, warm_start=False, \n",
    "                                              class_weight='balanced_subsample', n_estimators = 2955)\n",
    "\n",
    "# We create a dictionary with our models:\n",
    "estimators=[('RandomF', rf), ('RandomF2', rf2),('RandomF3', rf3)  ]\n",
    "#create our voting classifier, inputting our models\n",
    "ensemble = VotingClassifier(estimators, voting='soft')\n",
    "\n",
    "fitted_model = ensemble.fit(X, y)\n",
    "pred_dev = ensemble.predict_proba(X)[:,1]\n",
    "pred_oot  = ensemble.predict_proba(Xo)[:,1]\n",
    "\n",
    "dfo['pred'] = pred_oot\n",
    "dfo_tosend = dfo[list(['id','pred'])]\n",
    "i=1\n",
    "filename = \"student_sub\"+str(i)+\".csv\"\n",
    "dfo_tosend.to_csv(filename, sep=',')\n",
    "files = {'file': (filename, open(filename, 'rb'))}\n",
    "rsub = requests.post(url, files=files, auth=HTTPBasicAuth('mohamed.khanafer', 'mfalonso123'))\n",
    "resp_str = str(rsub.text)\n",
    "resp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Conclusion on the final model chosen\n",
    "As seen from the above section, the various approaches yielded different results and all can be summarized in the following table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HP KS2</th>\n",
       "      <th>HP Grade</th>\n",
       "      <th>HP + OV KS2</th>\n",
       "      <th>HP + OV Grade</th>\n",
       "      <th>HP + DUP + FS KS2</th>\n",
       "      <th>HP + DUP + FS Grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>XGBC Model 1</th>\n",
       "      <td>0.3340</td>\n",
       "      <td>7.916</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBC Model 2</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.3316</td>\n",
       "      <td>7.858</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBC Model 3</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.3518</td>\n",
       "      <td>8.337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CATB Model 1</th>\n",
       "      <td>0.3566</td>\n",
       "      <td>8.451</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CATB Model 2</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.3778</td>\n",
       "      <td>8.953</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CATB Model 3</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.3824</td>\n",
       "      <td>9.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF Model 1</th>\n",
       "      <td>0.3755</td>\n",
       "      <td>8.9</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF Model 2</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.3821</td>\n",
       "      <td>9.056</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF Model 3</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.4219</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ensemble: RF and CATB</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.5352</td>\n",
       "      <td>9.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ensemble: 3 RF</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.5381</td>\n",
       "      <td>9.981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       HP KS2 HP Grade HP + OV KS2 HP + OV Grade  \\\n",
       "XGBC Model 1           0.3340    7.916           -             -   \n",
       "XGBC Model 2                -        -      0.3316         7.858   \n",
       "XGBC Model 3                -        -           -             -   \n",
       "CATB Model 1           0.3566    8.451           -             -   \n",
       "CATB Model 2                -        -      0.3778         8.953   \n",
       "CATB Model 3                -        -           -             -   \n",
       "RF Model 1             0.3755      8.9           -             -   \n",
       "RF Model 2                  -        -      0.3821         9.056   \n",
       "RF Model 3                  -        -           -             -   \n",
       "Ensemble: RF and CATB       -        -           -             -   \n",
       "Ensemble: 3 RF              -        -           -             -   \n",
       "\n",
       "                      HP + DUP + FS KS2 HP + DUP + FS Grade  \n",
       "XGBC Model 1                          -                   -  \n",
       "XGBC Model 2                          -                   -  \n",
       "XGBC Model 3                     0.3518               8.337  \n",
       "CATB Model 1                          -                   -  \n",
       "CATB Model 2                          -                   -  \n",
       "CATB Model 3                     0.3824               9.062  \n",
       "RF Model 1                            -                   -  \n",
       "RF Model 2                            -                   -  \n",
       "RF Model 3                       0.4219                10.0  \n",
       "Ensemble: RF and CATB            0.5352                9.63  \n",
       "Ensemble: 3 RF                   0.5381               9.981  "
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame({'HP KS2':[\"0.3340\",\"-\",\"-\",\"0.3566\",\"-\",\"-\",\"0.3755\",\"-\",\"-\",\"-\",\"-\"],\n",
    "                        'HP Grade':[\"7.916\",\"-\",\"-\",\"8.451\",\"-\",\"-\",\"8.9\",\"-\",\"-\",\"-\",\"-\"],\n",
    "                        'HP + OV KS2': [\"-\",\"0.3316\",\"-\",\"-\",\"0.3778\",\"-\",\"-\",\"0.3821\",\"-\",\"-\",\"-\"],\n",
    "                        'HP + OV Grade': [\"-\",\"7.858\",\"-\",\"-\",\"8.953\",\"-\",\"-\",\"9.056\",\"-\",\"-\",\"-\"],\n",
    "                        'HP + DUP + FS KS2': [\"-\",\"-\",\"0.3518\",\"-\",\"-\",\"0.3824\",\"-\",\"-\",\"0.4219\",\"0.5352\",\"0.5381\"],\n",
    "                        'HP + DUP + FS Grade': [\"-\",\"-\",\"8.337\",\"-\",\"-\",\"9.062\",\"-\",\"-\",\"10.0\",\"9.63\",\"9.981\"]\n",
    "                       }, index=[\"XGBC Model 1\", \"XGBC Model 2\", \"XGBC Model 3\", \"CATB Model 1\", \"CATB Model 2\", \"CATB Model 3\",\"RF Model 1\",\"RF Model 2\", \"RF Model 3\",\"Ensemble: RF and CATB\",\"Ensemble: 3 RF\"])\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concluding Remark:\n",
    "As can be seen in the above table, the best model I got was a Random Forrest following the 3rd approach which is the Hyperparameter Optimization + Duplicated fraud + Feature Selection. \n",
    "And I would like to comment on an interesting aspect of this problem. All of these steps were performed manually. Indeed, as mentioned above, the hyper parameters that gave me the best results were those that I manually set. For the resampling approach, the manually duplicated features gave me the best results as well. And finally, for the feature selection, I manually selected the features based on the insights I got running the feature importance functions as well as noticing the high skewness in the float variables. Indeed, it is by excluding those from the model that I got the best results for my model. Given that the variables are not known and that this dataset is somehow a special one, the process I followed including a lot of manual decisions  is for sure not the most appropriate one when it comes to Machine Learning per se but I believe that it shows how for this problem, combining the insights generated from proper machine learning and with some creativity a data scientist can achieve great things. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
